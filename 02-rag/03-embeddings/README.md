# üî¢ M√≥dulo 3: Embeddings

> **Objetivo:** Transformar texto em n√∫meros (Coordenadas Sem√¢nticas).  
> **Status:** A funda√ß√£o da Busca Sem√¢ntica e do RAG.

## üìö Conte√∫do Pr√°tico (Scripts)

Nesta pasta, criamos 5 scripts para voc√™ entender embeddings do zero:

| Arquivo                             | O que voc√™ vai aprender?                                  |
|:------------------------------------|:----------------------------------------------------------|
| **`01_concept_vectors.py`**         | A matem√°tica b√°sica. O que √© um vetor? Como calcular dist√¢ncias? |
| **`02_openai_embeddings.py`**       | Como gerar vetores usando a API da OpenAI (padr√£o de mercado). |
| **`03_local_embeddings.py`**        | Como gerar vetores **de gra√ßa** e localmente com Open Source.   |
| **`04_semantic_search_demo.py`**    | **A M√°gica do RAG!** Criando um mini-buscador sem√¢ntico.       |
---

## üöÄ Como Executar

### 1. Instalar Depend√™ncias
Certifique-se de que voc√™ tem o Python instalado e rode:
```bash
pip install -r requirements.txt
```

### 2. Configurar OpenAI (Opcional para scripts locais)
Para rodar o script `02`, voc√™ precisa de uma chave API da OpenAI no arquivo `.env`:
```env
OPENAI_API_KEY=sk-...
```

### 3. Rodar os Scripts
Basta executar com python:
```bash
python 01_concept_vectors.py
python 04_semantic_search_demo.py
# etc...
```

---

## üß† Teoria Resumida (O que voc√™ precisa saber)

### 1. O Problema: Computadores s√£o Ru√≠ns com Texto
Computadores n√£o entendem palavras, apenas n√∫meros.
- Antigamente, us√°vamos IDs (Cachorro=1, Lobo=2, Banana=3).
- **O defeito:** Matematicamente, 2 (Lobo) n√£o √© "mais pr√≥ximo" de 1 (Cachorro) do que 3 (Banana). Perdemos o significado.

### 2. A Solu√ß√£o: Vetores de Caracter√≠sticas (Embeddings)
Em vez de um n√∫mero, usamos uma **lista de n√∫meros**. Cada n√∫mero representa uma caracter√≠stica (ou **Dimens√£o**).
Imagine um gr√°fico "Fofura vs Tamanho":
- üê∂ Cachorro: `[0.9, 0.4]` (Muito fofo, pequeno)
- üê∫ Lobo:     `[0.1, 0.5]` (Pouco fofo, m√©dio)
- üçå Banana:   `[0.0, 0.1]` (Nada fofo, pequeno)

Agora, matematicamente, o Lobo est√° perto do Cachorro!

### 3. O que s√£o Dimens√µes?
Nos exemplos did√°ticos, usamos 2 ou 3 dimens√µes.
Na vida real, modelos como o da OpenAI usam **1536 dimens√µes**.
- Cada dimens√£o captura uma nuance sutil da linguagem (g√™nero, pluralidade, sentimento, contexto, etc).
- **Mais Dimens√µes** = Mais intelig√™ncia e nuance.
- **Menos Dimens√µes** = Mais r√°pido e barato.

### 4. Como eles s√£o calculados?
Ningu√©m escreve esses n√∫meros √† m√£o. Eles s√£o **Treinados** por Redes Neurais.
A IA l√™ a internet inteira tentando adivinhar a pr√≥xima palavra.
- *"O gato subiu no..."* (Telhado? √Årvore? Batata?)
- Se ela acerta, ela ajusta os n√∫meros.
- No final, palavras usadas em contextos parecidos acabam tendo n√∫meros parecidos.

---

### 5. Escolha do Modelo (Guia R√°pido 2025)

| Modelo | Provedor | Dims | Pros | Contras |
|:---|:---|:---|:---|:---|
| **text-embedding-3-small** | OpenAI | 1536 | Barato, r√°pido, padr√£o. | Privacidade, Custo em escala. |
| **text-embedding-3-large** | OpenAI | 3072 | Maior acur√°cia. | 2x custo, 2x tamanho de storage. |
| **bge-m3 / multilingual-e5** | Open Source | 1024 | Gr√°tis, roda local, bate a OpenAI. | Voc√™ precisa hospedar (GPU necess√°ria). |

---

## ‚è≠Ô∏è Pr√≥ximo Passo
Agora que voc√™ entendeu que embeddings s√£o apenas **coordenadas de significado instru√≠das por leitura massiva**, onde guardamos esses milh√µes de vetores?

V√° para **[M√≥dulo 4: Vector Databases](../04-vector-dbs)**.
