# üì• M√≥dulo 02: Ingestion Pipeline

> **Goal:** "Garbage In, Garbage Out". Se o seu RAG falha, 80% das vezes a culpa √© do Ingestion.
> 
> **Foco:** Transformar documentos n√£o estruturados (PDF, HTML) em peda√ßos de texto semanticamente √∫teis (Chunks).

---

## üõ†Ô∏è O Pipeline de Ingest√£o

Antes de embeddar e salvar no Qdrant, precisamos preparar os dados.

1.  **Parsing (Extra√ß√£o):** Tira o texto do arquivo.
2.  **Chunking (Divis√£o):** Quebra o texto em peda√ßos que cabem no contexto do LLM.

### üìÇ Scripts deste M√≥dulo

| Arquivo | T√≥pico | Descri√ß√£o |
| :--- | :--- | :--- |
| **[01_text_extraction_pypdf.py](./01_text_extraction_pypdf.py)** | **Raw Text** | Extra√ß√£o simples e r√°pida com `pypdf`. Perde tabelas e layout. |
| **[02_layout_parsing_docling.py](./02_layout_parsing_docling.py)** | **Layout Aware** | Extra√ß√£o inteligente com `Docling` (preserva tabelas e headers). |
| **[03_chunking_recursive.py](./03_chunking_recursive.py)** | **Recursive** | O chunker padr√£o do LangChain. Bom para textos gerais. |
| **[04_chunking_token.py](./04_chunking_token.py)** | **Token Limit** | Garante que o chunk respeita o limite do modelo (ex: OpenAI `tiktoken`). |
| **[05_chunking_markdown.py](./05_chunking_markdown.py)** | **Structure** | Usa headers Markdown (#, ##) como fronteira sem√¢ntica. O melhor para docs t√©cnicos. |

---

## üß† Parsing: Texto "Burro" vs Layout "Inteligente"

*   **Pypdf:** S√≥ l√™ strings. Se tiver uma tabela com duas colunas, ele pode ler a linha 1 da col 1 e depois a linha 1 da col 2, misturando tudo.
*   **Docling / Unstructured:** Entendem que aquilo √© uma tabela. Convertem para Markdown ou JSON estruturado, preservando a rela√ß√£o entre os dados.

> **Regra de Ouro:** Para contratos, relat√≥rios financeiros e papers cient√≠ficos, use Layout Parsing. Para e-mails ou textos simples, use extra√ß√£o b√°sica.

---

## ‚úÇÔ∏è Chunking Strategies

N√£o existe "tamanho ideal de chunk". Existe o tamanho certo para sua pergunta.

1.  **Chunks Pequenos (128-256 tokens):** √ìtimos para perguntas precisas ("Qual a data do contrato?"). Perdem contexto amplo.
2.  **Chunks Grandes (512-1024 tokens):** √ìtimos para resumo ou perguntas gerais ("Sobre o que √© o documento?"). Custa mais e pode confundir a busca (muito ru√≠do).
3.  **Semantic Chunking:** Quebra onde o assunto muda (avan√ßado).
4.  **Markdown/Structure Chunking:** Quebra por se√ß√£o l√≥gica (Introdu√ß√£o, Conclus√£o).

---
## Explica√ß√£o do M√≥dulo 2

> **Goal:** Lixo entra, Lixo sai. Domine a arte de limpar dados.  
> **Status:** A parte mais subestimada da IA.

## 1. O Documento √© o Inimigo
PDFs s√£o feitos para impress√£o, n√£o para leitura.
- Eles t√™m cabe√ßalhos, rodap√©s, colunas m√∫ltiplas e imagens.
- Se voc√™ extrair texto cegamente, recebe: `Cabe√ßalho Pag 1 Conte√∫do Cabe√ßalho Pag 2`.
- Isso destr√≥i o significado sem√¢ntico.

### Estrat√©gias de Parsing
1.  **Text Extraction (pypdf):** R√°pido, gr√°tis, perde tabelas/layout. Use para contratos simples.
2.  **OCR (Tesseract):** Essencial para docs escaneados. Lento.
3.  **Vision Models (GPT-4o / Claude Vision):** Envia a imagem da p√°gina. Caro, mas 99% preciso.
4.  **Layout Parsing (Unstructured.io / Microsoft Azure DI):** Detecta "T√≠tulo", "Tabela", "Barra Lateral". A escolha profissional.

## 2. Filosofia de Chunking
Voc√™ n√£o pode enviar um livro de 100 p√°ginas para o modelo de embedding (contexto limitado). Voc√™ deve "fatiar" (chunk). [Text splitter Langchain Docs](https://docs.langchain.com/oss/python/integrations/splitters)

### Estrat√©gia A: Fixed Size (O jeito "ing√™nuo")
- Dividir a cada 500 caracteres.
- **Problema:** Corta frases no meio. Quebra contexto.

### Estrat√©gia B: Recursive Character (Padr√£o LangChain)
- Divide por Par√°grafos (`\n\n`) -> Frases (`.`) -> Palavras (` `).
- **Veredito:** Bom baseline.

### Estrat√©gia C: Semantic Chunking (Avan√ßado)
- Usa um modelo de embedding para escanear o documento.
- Inicia um novo chunk quando o *t√≥pico muda* (similaridade de cosseno cai).
- **Veredito:** Alta qualidade, indexa√ß√£o mais lenta.

### Estrat√©gia D: Hierarchical Indexing (Parent-Child)
- **Store:** A p√°gina inteira (Pai).
- **Search:** Pequenos chunks de 200 chars (Filhos).
- **Retrieval:** Se um filho √© encontrado, retorne o *Pai*.
- **Por que:** Chunks pequenos casam melhor com a busca. Chunks grandes d√£o melhor contexto pro LLM.

## 3. Extra√ß√£o de Metadados
**Se voc√™ n√£o extrai metadados, sua busca √© burra.**

Exemplo: "Qual foi a receita em 2023?"
- **Sem Metadados:** Busca em todos os docs "receita". Retorna 2021, 2022, 2024.
- **Com Metadados:** Filtra `year == 2023`.

**Como extrair?**
- Use um LLM barato (GPT-4o-mini) durante a ingest√£o para extrair JSON:
  ```json
  {
    "title": "Relat√≥rio Q3",
    "year": 2023,
    "department": "Vendas",
    "summary": "Receita subiu 20%"
  }
  ```

## 4. Arquitetura Real de Pipeline
N√£o escreva um script. Construa um pipeline.

1.  **Trigger:** Usu√°rio sobe arquivo.
2.  **Queue:** Arquivo vai para Redis/SQS.
3.  **Worker:**
    - Detecta tipo (MIME).
    - Parse (Unstructured).
    - Extrai Metadados (LLM).
    - Chunk (Recursive).
    - Embed (OpenAI).
    - Upsert (Qdrant).
4.  **Status:** Notifica Usu√°rio "Arquivo Pronto".

## üß† Mental Model: "Fragmenta√ß√£o"
Se voc√™ picotar um romance de mist√©rio aleatoriamente, pode pegar um peda√ßo que diz:
*"Ele fez isso."*
Quem √© "Ele"? O contexto foi perdido.
**Overlap** ajuda (manter 50 chars do anterior), mas **Parent-Child** √© a corre√ß√£o real.

## ‚è≠Ô∏è Pr√≥ximo Passo
Como transformamos texto em matem√°tica?
V√° para **[M√≥dulo 3: Embeddings](../03-embeddings)**.
