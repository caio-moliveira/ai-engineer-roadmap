# 08 - Evaluation de RAG com RAGAS

## O que √© Avalia√ß√£o de RAG?

Avaliar um sistema RAG (Retrieval-Augmented Generation) √© crucial para garantir que ele n√£o apenas recupere os documentos certos, mas tamb√©m gere respostas precisas e √∫teis baseadas neles.

Diferente de tarefas tradicionais de NLP, no RAG precisamos avaliar dois componentes principais independentente e em conjunto:

1.  **Componente de Recupera√ß√£o (Retriever):** "Eu encontrei os documentos certos?"
2.  **Componente de Gera√ß√£o (Generator/LLM):** "Eu respondi a pergunta corretamente usando os documentos encontrados?"

Para isso, utilizamos frameworks como o **RAGAS** (RAG Assessment), que oferece m√©tricas padronizadas para quantificar a qualidade do seu pipeline.

## Principais M√©tricas do RAGAS

O [RAGAS](https://docs.ragas.io/en/stable/tutorials/rag/) prop√µe m√©tricas que cobrem diferentes aspectos do RAG. As quatro principais s√£o:

### 1. Faithfulness (Fidelidade)
*   **O que mede:** Se a resposta gerada pode ser inferida **apenas** a partir do contexto recuperado.
*   **Por que importa:** Evita alucina√ß√µes. Garante que o modelo n√£o est√° inventando informa√ß√µes que n√£o est√£o nos documentos.
*   **Pergunta chave:** "A resposta 'respeita' o contexto fornecido?"

### 2. Answer Relevance (Relev√¢ncia da Resposta)
*   **O que mede:** O qu√£o relevante a resposta gerada √© para a **pergunta original** (prompt).
*   **Por que importa:** Garante que o modelo n√£o est√° tangenciando ou ignorando a pergunta do usu√°rio.
*   **Pergunta chave:** "A resposta ataca diretamente a d√∫vida do usu√°rio?"

### 3. Context Precision (Precis√£o do Contexto)
*   **O que mede:** A propor√ß√£o de chunks **relevantes** dentre os chunks recuperados.
*   **Por que importa:** (Avalia√ß√£o do Retriever) Mede se estamos trazendo muito lixo junto com a informa√ß√£o √∫til.
*   **Pergunta chave:** "Quanto do que eu recuperei √© realmente √∫til?"

### 4. Context Recall (Revoca√ß√£o do Contexto)
*   **O que mede:** Se o contexto recuperado cont√©m **toda** a informa√ß√£o necess√°ria para responder a uma "Ground Truth" (resposta ideal esperada).
*   **Por que importa:** (Avalia√ß√£o do Retriever) Mede se deixamos passar alguma informa√ß√£o importante.
*   **Nota:** Exige um dataset com `ground_truth` (respostas corretas esperadas).

---

## üîç Observabilidade com Langfuse

Para garantir o bom funcionamento do nosso sistema RAG em ambiente produtivo, dependemos fortemente de pr√°ticas de **observabilidade**. A observabilidade nos permite medir, rastrear e depurar o comportamento de agentes e LLMs de forma escal√°vel. Neste m√≥dulo, utilizamos o **[Langfuse](https://langfuse.com/docs)**, uma plataforma de engenharia de LLM open-source.

O Langfuse nos oferece uma vis√£o completa sobre todas as etapas do nosso pipeline, desde a ingest√£o de metadados dos documentos no banco vetorial at√© as chamadas de ferramentas (*tool calling*) feitas pelo modelo.

Abaixo explicamos como ele foi integrado em nosso projeto:

### 1. Ingest√£o e Indexa√ß√£o (`utils.py`)
No est√°gio de prepara√ß√£o do banco vetorial, utilizamos o cliente nativo do Langfuse para rastrear detalhadamente o passo a passo atrav√©s de identificadores chamados de `spans`:

*   **Traces Hier√°rquicos:** Iniciamos uma observa√ß√£o raiz (`start_as_current_observation`) para englobar toda a execu√ß√£o da fun√ß√£o `load_and_index_pdf`.
*   **Detalhamento de Etapas (Spans):** Criamos *spans* filhos para monitorar os tempos de execu√ß√£o e o status das sub-tarefas: `check_collection`, `load_pdf`, `chunking`, `qdrant_prepare` e `qdrant_upsert`.
*   **Enriquecimento de Dados:** Usamos `propagate_attributes` para injetar tags e metadados relevantes √† ingest√£o (como tamanho dos *chunks* configurado, `PDF_PATH` e par√¢metros do Qdrant) nas observa√ß√µes geradas, facilitando a busca no painel do Langfuse. Tamb√©m registramos os *outputs* durante a execu√ß√£o de cada *span* usando `update_current_span()`.

### 2. Gera√ß√£o e Agente (`01_rag_agent_eval.py`)
Na etapa de execu√ß√£o da l√≥gica do RAG (Agentic RAG), aproveitamos a [integra√ß√£o nativa do Langfuse com o LangChain](https://langfuse.com/docs/integrations/langchain) para simplificar a coleta de informa√ß√µes (*traces*):

*   **Callback Handler:** Instanciamos o `CallbackHandler` espec√≠fico atrav√©s do m√≥dulo `langfuse.langchain`.
*   **Rastreamento Autom√°tico:** Passando este *handler* no par√¢metro de configura√ß√£o (`config={"callbacks": [langfuse_handler]}`) durante a invoca√ß√£o do nosso agente (`agent_executor.stream`), o Langfuse automaticamente captura e desenha uma √°rvore completa da execu√ß√£o.
*   **Visibilidade de Ferramentas:** Sempre que a ferramenta customizada `retrieve_context` ou o fallback para busca online (`DuckDuckGoSearchRun`) √© acionado pelo provedor do LLM, os fluxos de entrada (*query*) e sa√≠da (quais documentos foram recuperados do vector store), assim como m√©tricas de precis√£o e lat√™ncia, s√£o gravados automaticamente e enviados para o servidor de observabilidade.

Para configurar o Langfuse no seu projeto, lembre-se de configurar as suas credenciais de autentica√ß√£o (`LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, e `LANGFUSE_HOST`) criadas na plataforma e inclu√≠-las no seu arquivo `.env` na raiz do projeto.

---

## Como Executar

### Pr√©-requisitos

Certifique-se de ter as depend√™ncias instaladas:

```bash
uv add ragas datasets langchain-openai langchain-qdrant qdrant-client
```

### Script de Avalia√ß√£o

O script `01_ragas_evaluation.py` demonstra como criar um dataset simples de perguntas e respostas geradas pelo nosso RAG e avali√°-las usando as m√©tricas acima.

**Nota:** O script reutiliza a fun√ß√£o `load_and_index_pdf` do m√≥dulo `06-rag-agent` para subir o banco vetorial.

```bash
python 01_ragas_evaluation.py
```

Isso ir√°:
1.  Carregar o PDF e indexar no Qdrant (se necess√°rio).
2.  Executar um mini-pipeline de RAG para 3 perguntas de exemplo sobre o documento.
3.  Coletar: `question`, `answer`, `contexts`.
4.  Executar a avalia√ß√£o do RAGAS e exibir os scores.
