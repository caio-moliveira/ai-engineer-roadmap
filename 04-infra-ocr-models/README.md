# üîπ Bloco 4: Infra, OCR e Modelos (Infer√™ncia em Produ√ß√£o)

> **Objetivo:** Rodar modelos de forma eficiente, barata e confi√°vel.  
> **Status:** Onde a engenharia de software encontra o "Metal".

## üõë Pare. Leia isto.
Este bloco n√£o √© sobre "como treinar modelos".  
√â sobre **como operar modelos**.

A maioria dos projetos de IA morre porque:
1.  **Custo:** A conta da AWS/OpenAI torna o produto invi√°vel.
2.  **Lat√™ncia:** O usu√°rio desiste de esperar 10 segundos pela resposta.
3.  **Dados Sujos:** O pipeline de OCR falha em ler o PDF do cliente.

Aqui voc√™ vai aprender a ser um **Engenheiro de Infraestrutura de IA**.

---

## üìö Ementa do M√≥dulo

### [M√≥dulo 1: Ecossistema Moderno de Modelos](./01-model-ecosystem)
- **Decis√£o:** API Propriet√°ria (OpenAI/Anthropic) vs Open Source (Llama/Mistral).
- **Crit√©rios:** Privacidade, Lat√™ncia, Custo e Complexidade Operacional.
- **Estrat√©gia:** "Good Enough" models e padr√µes de roteamento.

### [M√≥dulo 2: Ecossistema Hugging Face](./02-hugging-face)
- **Abase:** O que s√£o Safetensors, Tokenizers e Transformers na pr√°tica.
- **Formatos:** FP16, INT8, GGUF, AWQ. O que usar e quando.
- **Realidade:** Quando o Hugging Face √© essencial e quando √© complexidade desnecess√°ria.

### [M√≥dulo 3: Ollama (Dev Locals)](./03-ollama)
- **Prototipagem:** Como rodar Llama 3 no seu MacBook em 5 minutos.
- **Limites:** Por que voc√™ (provavelmente) n√£o deve usar Ollama em produ√ß√£o de alta escala.
- **Workflow:** De local (Ollama) para staging (vLLM).

### [M√≥dulo 4: vLLM (Infer√™ncia de Produ√ß√£o)](./04-vllm)
- **O Padr√£o:** Continuous Batching e PagedAttention.
- **Servindo:** Como subir um servidor compat√≠vel com OpenAI API que aguenta 1000 requests/seg.
- **Tunning:** Ajustando KV Cache e Max Tokens para throughput m√°ximo.

### [M√≥dulo 5: Hardware & Performance](./05-hardware-performance)
- **VRAM is King:** Por que a mem√≥ria da GPU importa mais que o Compute.
- **Unit Economics:** Quanto custa 1 milh√£o de tokens self-hosted vs API?
- **Quantiza√ß√£o:** As trocas entre precis√£o e velocidade.

### [M√≥dulo 6: Fundamentos de OCR](./06-ocr-fundamentals)
- **A Mentira:** OCR n√£o √© apenas extrair texto. √â extrair layout, tabelas e estrutura.
- **Desafios:** Rota√ß√£o, ru√≠do, caligrafia e formata√ß√£o complexa.
- **M√©tricas:** Quando CER/WER importam e quando s√£o irrelevantes.

### [M√≥dulo 7: Frameworks e Pipelines de OCR](./07-ocr-pipelines)
- **Ferramentas:** Tesseract vs Azure DI vs Vision LLMs (GPT-4o).
- **Arquitetura:** Pr√©-processamento, OCR, P√≥s-processamento e Chunking.
- **Tradeoffs:** Custo (Vision LLM) vs Qualidade vs Velocidade.

### [M√≥dulo 8: Document Intelligence em Produ√ß√£o](./08-document-intelligence)
- **End-to-End:** Ingest√£o, Fila (SQS), Processamento Idempotente e Indexa√ß√£o.
- **Falhas:** Dead Level Queues e estrat√©gias de retry.
- **Monitoramento:** Como saber se o seu pipeline de PDF parou.

---

## üõ†Ô∏è Stack de Infra (Padr√£o 2025)

| Componente | Escolha | Por qu√™? |
|:---|:---|:---|
| **Infer√™ncia Local** | Ollama | DX imbat√≠vel para desenvolvimento. |
| **Infer√™ncia Prod** | vLLM | Padr√£o ouro para throughput em GPUs NVIDIA. |
| **Model Registry** | Hugging Face | O GitHub dos modelos. |
| **Container** | Docker (NVIDIA Runtime) | Isolamento e portabilidade. |
| **OCR** | H√≠brido (Layout Parser + Vision LLM) | Melhor custo-benef√≠cio para documentos complexos. |

## üß† Mudan√ßas Mentais Necess√°rias
- **GPU n√£o √© CPU:** O gargalo quase sempre √© largura de banda de mem√≥ria (VRAM Bandwidth), n√£o FLOPs.
- **Pipeline > Modelo:** Um modelo m√©dio com um pipeline de dados excelente bate um modelo state-of-the-art com dados ruins.
- **Ass√≠ncrono √© Obrigat√≥rio:** Modelos s√£o lentos. OCR √© lento. Se seu sistema for s√≠ncrono, ele vai cair.

## üöÄ Como come√ßar
V√° para **[M√≥dulo 1: Ecossistema Moderno de Modelos](./01-model-ecosystem)**.
