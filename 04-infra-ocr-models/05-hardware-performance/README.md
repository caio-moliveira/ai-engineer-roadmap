# âš¡ MÃ³dulo 5: Hardware & Performance Fundamentals

> **Goal:** NÃ£o quebrar o banco.  
> **Status:** Onde a mÃ¡gica encontra a fÃ­sica.

## 1. VRAM is King
EsqueÃ§a TFLOPS. O gargalo Ã© **Memory Bandwidth**.
Um modelo de 70 bilhÃµes de parÃ¢metros (Llama-3-70B) em FP16 precisa de ~140GB de VRAM sÃ³ para carregar.
- **A100 (80GB):** Cabe metade.
- **H100 (80GB):** Cabe metade.
- **RTX 4090 (24GB):** Nem sonhando.

## 2. QuantizaÃ§Ã£o: Int8/Int4
Para rodar 70B em hardware "mortal", usamos quantizaÃ§Ã£o.
- **FP16:** 140GB
- **INT8:** 70GB (Cabe em 1x A100)
- **INT4:** 35GB (Cabe em 2x 4090)

**Tradeoff:** INT4 tem perda de qualidade quase imperceptÃ­vel para tarefas gerais, mas pode falhar em raciocÃ­nio complexo.

## 3. Unit Economics (Custo de InferÃªncia)
A mÃ©trica que importa Ã©: **$ / 1M tokens**.
- **GPT-4o:** ~$5.00
- **Llama-3-70B (Self-Host AWS):** ~$0.50 (se vocÃª tiver 100% de utilizaÃ§Ã£o).
- **Llama-3-70B (Self-Host AWS):** ~$50.00 (se vocÃª tiver 1% de utilizaÃ§Ã£o).

> **Alerta:** Self-hosting sÃ³ Ã© mais barato se vocÃª tiver **trÃ¡fego massivo e constante** para manter a GPU ocupada.

## ğŸ§  Mental Model: "Aluguel vs Hipoteca"
- **API (Token-based):** Uber. VocÃª paga pelo km rodado. Caro por km, mas zero custo parado.
- **Self-Host (GPU):** Carro PrÃ³prio. VocÃª paga a parcela todo mÃªs, usando ou nÃ£o. SÃ³ vale a pena se rodar muito.

## â­ï¸ PrÃ³ximo Passo
Vamos falar de dados sujos.
VÃ¡ para **[MÃ³dulo 6: Fundamentos de OCR](../06-ocr-fundamentals)**.
