# ü¶ô M√≥dulo 3: Ollama (Prototipagem Local R√°pida)

> **Goal:** DX (Developer Experience) perfeita.  
> **Status:** O "Docker" dos LLMs.

## 1. Por que Ollama?
Antes do Ollama: Configurar Python, Cuda, Torch, compilar llama.cpp... (4 horas).
Com Ollama:
```bash
ollama run llama3
```
(5 minutos).

## 2. Quando usar?
- **Desenvolvimento Local:** Testar prompts sem gastar $ API.
- **Air-gapped demos:** Mostrar IA em um laptop sem internet.
- **CI/CD:** Rodar testes de integra√ß√£o leves.

## 3. Quando N√ÉO usar?
**Produ√ß√£o de Alta Escala.**
Ollama foca em *usabilidade*, n√£o em *throughput* m√°ximo (embora esteja melhorando).
Para servir 10.000 usu√°rios, voc√™ quer controle total sobre o Batching, o que o **vLLM** oferece melhor.

## 4. Workflow de Engenharia
1.  **Dev (MacBook):** Desenvolve usando Ollama (`llama3:8b`).
2.  **Teste:** Valida prompts e tools.
3.  **Staging/Prod (GPU Server):** Deploy usando cont√™iner Docker oficial do vLLM apontando para o mesmo modelo.

## üß† Mental Model: "SQLite vs PostgreSQL"
- **Ollama √© SQLite:** F√°cil, arquivo √∫nico, √≥timo para dev e apps leves.
- **vLLM √© PostgreSQL:** Robusto, configur√°vel, feito para aguentar o tranco de uma empresa inteira.

## ‚è≠Ô∏è Pr√≥ximo Passo
Vamos falar do PostgreSQL dos LLMs.
V√° para **[M√≥dulo 4: vLLM](../04-vllm)**.
