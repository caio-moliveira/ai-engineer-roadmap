# üöÄ M√≥dulo 4: vLLM (Core de Infer√™ncia)

> **Goal:** Extrair m√°ximo de tokens por segundo da sua GPU.  
> **Status:** O padr√£o industrial para self-hosting.

## 1. Por que vLLM?
Servir LLMs √© dif√≠cil por causa da mem√≥ria.
O **KV Cache** (mem√≥ria da conversa) cresce e diminui dinamicamente.
Engines antigas alocavam mem√≥ria est√°tica (desperdi√ßavam VRAM).
**vLLM** introduziu **PagedAttention** (inspirado em mem√≥ria virtual de OS), permitindo ocupar 95% da VRAM com efici√™ncia.

**Resultado:** 20x mais throughput que HuggingFace Transformers padr√£o.

## 2. Conceitos Core
- **Continuous Batching:** N√£o espera um pedido terminar para come√ßar outro. Encaixa novos pedidos nos "buracos" de processamento.
- **Quantization (AWQ/GPTQ):** vLLM roda modelos quantizados de forma super otimizada.

## 3. Pattern de Deploy (Docker)
N√£o instale vLLM no bare metal. Use Docker.

```yaml
services:
  llm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=...
    command: --model meta-llama/Meta-Llama-3-8B-Instruct --quantization awq --max-model-len 4096
    ports:
      - "8000:8000"
```

## 4. Confiabilidade em Produ√ß√£o
- **Max Tokens:** Limite isso! Se um usu√°rio pedir 1 milh√£o de tokens, seu servidor trava.
- **Timeout:** Configure timeouts agressivos no cliente.
- **Load Shedding:** Se a fila estiver cheia (HTTP 503), rejeite novos requests imediatamente para n√£o derrubar o servi√ßo.

## üß† Mental Model: "O √înibus Lotado"
O Continuous Batching √© como um √¥nibus.
Em vez de esperar o √¥nibus esvaziar para pegar novos passageiros, o vLLM deixa gente entrar e sair em cada ponto (token). O √¥nibus est√° sempre cheio (GPU sempre em 100%), maximizando a efici√™ncia.

## ‚è≠Ô∏è Pr√≥ximo Passo
Quanto custa esse √¥nibus?
V√° para **[M√≥dulo 5: Hardware & Performance](../05-hardware-performance)**.
