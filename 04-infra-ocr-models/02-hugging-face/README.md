# ü§ó M√≥dulo 2: Ecossistema Hugging Face (Pr√°tico)

> **Goal:** O GitHub da IA.  
> **Status:** Essencial para quem n√£o usa apenas APIs.

## 1. O que o HF realmente oferece?
N√£o √© s√≥ um site para baixar modelos. √â a infraestrutura padr√£o.
- **Transformers:** A lib Python padr√£o para rodar qualquer modelo.
- **Tokenizers:** Quem transforma texto em n√∫meros.
- **Safetensors:** O formato de arquivo seguro (sem pickle/execu√ß√£o de c√≥digo).

## 2. Model Formats (O que quebra?)
Voc√™ baixou um modelo e deu erro de RAM. Por qu√™?

- **FP32 (Float 32):** Precis√£o total. Gigante. (4 bytes por par√¢metro). Ningu√©m usa em infer√™ncia.
- **FP16 / BF16:** Padr√£o de treino/infer√™ncia. (2 bytes por par√¢metro).
- **INT8 / INT4 (Quantiza√ß√£o):** Compress√£o absurda. Perda m√≠nima de qualidade. Essencial para rodar em GPUs "normais".

## 3. GGUF (O formato Local)
Criado por Georgi Gerganov (llama.cpp).
- **Goal:** Rodar em CPU + Apple Silicon.
- **Como funciona:** Mapeia o modelo direto na mem√≥ria (mmap).
- **Uso:** Use com Ollama ou LM Studio. N√£o use em produ√ß√£o de alta performance (prefira AWQ/GPTQ em vLLM).

## 4. O Checklist "HF em Produ√ß√£o"
Se for baixar do HF para produ√ß√£o:
- [ ] Use `.safetensors` (nunca `.bin` ou `.pt` se poss√≠vel, risco de seguran√ßa).
- [ ] Verifique a licen√ßa (Apache 2.0 / MIT vs Llama Community).
- [ ] Cache: Configure `HF_HOME` para n√£o lotar seu disco de OS.

## üß† Mental Model: "Pesos Congelados"
Um modelo no HF √© um arquivo est√°tico de pesos (bilh√µes de n√∫meros).
Para "rodar", voc√™ precisa de uma **Engine** (vLLM, Ollama, Transformers) que carrega esses n√∫meros na VRAM e faz as contas de matrizes.

## ‚è≠Ô∏è Pr√≥ximo Passo
Como rodar isso localmente sem dor de cabe√ßa?
V√° para **[M√≥dulo 3: Ollama](../03-ollama)**.
