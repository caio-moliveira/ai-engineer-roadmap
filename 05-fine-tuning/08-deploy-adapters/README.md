# üöÄ M√≥dulo 8: Deploy & Infer√™ncia P√≥s-Treino

> **Goal:** Servir o modelo customizado.  
> **Status:** Onde o Adapter brilha.

## 1. LoRA Adapters no vLLM
Voc√™ n√£o precisa subir um servidor vLLM dedicado de 70GB para cada cliente.
O vLLM suporta **Multi-LoRA Serving**.
- Servidor: Carrega o Llama-3-70B Base (1 vez).
- Request A: `model="cliente_juridico"` -> vLLM aplica Adapter A on-the-fly.
- Request B: `model="cliente_medico"` -> vLLM aplica Adapter B on-the-fly.
**Lat√™ncia extra:** Quase zero.

## 2. Canary Deployment
Nunca troque o modelo base por um finetunado de uma vez.
Mande 1% do tr√°fego para o modelo novo.
Monitore:
- Taxa de erro (JSON inv√°lido?)
- Tamanho da resposta (Ficou verboso?)
- Feedback do usu√°rio (Thumbs down?).

## 3. Rollback
Se o modelo novo come√ßar a alucinar:
Em vLLM, basta parar de enviar requests com o par√¢metro `lora_name`. O modelo base continua l√°, intacto.

## üß† Mental Model: "Hot Swapping"
Gra√ßas aos Adapters, trocar o comportamento do modelo em produ√ß√£o √© t√£o r√°pido quanto trocar uma vari√°vel de ambiente. N√£o requer restart do servidor.

## ‚è≠Ô∏è Pr√≥ximo Passo
O que pode dar errado a longo prazo?
V√° para **[M√≥dulo 9: Riscos, Falhas & Manuten√ß√£o](../09-risks-maintenance)**.
