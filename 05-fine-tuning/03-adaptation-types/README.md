# üß¨ M√≥dulo 3: Tipos de Adapta√ß√£o (PEFT/LoRA)

> **Goal:** Treinar modelos gigantes em GPUs mortais.  
> **Status:** O estado da arte da efici√™ncia.

## 1. Full Fine-Tuning (O jeito antigo)
Atualiza todos os 70 bilh√µes de par√¢metros.
- **Problema:** Requer centenas de GBs de mem√≥ria (v√°rios clusters A100).
- **Risco:** Catastrophic Forgetting alto. O modelo esquece o que sabia antes.
- **Uso:** Quase nunca, a menos que voc√™ seja a OpenAI ou Google.

## 2. PEFT (Parameter-Efficient Fine-Tuning)
Congela o modelo base. Treina apenas pequenas camadas extras.

### LoRA (Low-Rank Adaptation)
Injeta pequenas matrizes trein√°veis nas camadas do modelo.
- **Tamanho:** O "adapter" final tem ~100MB.
- **Vantagem:** Voc√™ pode ter 1 modelo base e 50 adapters (um para SQL, um para Poesia, um para Jur√≠dico).
- **Custo:** Treina em uma √∫nica GPU consumer (RTX 3090/4090).

### QLoRA (Quantized LoRA)
Carrega o modelo base em 4-bit (perda m√≠nima) e treina o LoRA em cima.
- **Revolu√ß√£o:** Permite treinar Llama 3 70B em uma √∫nica GPU A100 (80GB) ou 2x RTX 3090.

## 3. Instruction Tuning vs Preference Tuning
- **Instruction Tuning (SFT - Supervised Fine-Tuning):**
    - Dataset: Pergunta -> Resposta Correta.
    - Goal: Ensinar a seguir instru√ß√µes.
- **Preference Tuning (DPO / ORPO):**
    - Dataset: Pergunta -> Resposta Boa vs Resposta Ruim.
    - Goal: Alinhar o modelo com prefer√™ncias humanas (evitar toxicidade, verbosidade).

## üß† Mental Model: "O Plugin"
Pense no LoRA como um arquivo DLC de um jogo.
O jogo base (Llama 3) tem 70GB.
O DLC (LoRA Adapter) tem 100MB e muda as roupas e di√°logos dos personagens.

## ‚è≠Ô∏è Pr√≥ximo Passo
O segredo n√£o √© o algoritmo. √â o dataset.
V√° para **[M√≥dulo 4: Dados s√£o o Modelo](../04-data-prep)**.
