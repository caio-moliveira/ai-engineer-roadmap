# üîπ Bloco 5: Fine-Tuning e Melhora de Modelo

> **Objetivo:** Saber quando treinar ‚Äî e principalmente quando N√ÉO treinar.  
> **Status:** A √∫ltima milha da especializa√ß√£o.

## üõë Pare. Leia isto.
Fine-Tuning n√£o resolve alucina√ß√£o.
Fine-Tuning n√£o adiciona conhecimento factual novo de forma confi√°vel.
Fine-Tuning n√£o √© m√°gica.

Se voc√™ est√° aqui porque "o RAG n√£o funcionou", volte para o Bloco 2.
Fine-Tuning √© para **Forma**, **Estilo**, **Comportamento** e **Efici√™ncia**, n√£o para fatos.

Este bloco vai te ensinar a responsabilidade de "tocar nos pesos" do modelo.

---

## üìö Ementa do M√≥dulo

### [M√≥dulo 1: O que √© Fine-Tuning (Realmente)](./01-finetuning-concepts)
- **Realidade:** Adapta√ß√£o de pesos vs Inje√ß√£o de Conhecimento.
- **Mito:** "Vou treinar o modelo nos meus PDFs para ele saber sobre minha empresa." (Spoiler: N√£o vai funcionar).
- **Fato:** Fine-Tuning ensina o modelo a FALAR como um m√©dico, n√£o a SER um m√©dico.

### [M√≥dulo 2: Fine-Tuning vs RAG vs Prompting](./02-rag-vs-finetuning)
- **Matriz de Decis√£o:** O framework definitivo para escolher a abordagem.
- **RAG:** Para fatos novos e din√¢micos.
- **Fine-Tuning:** Para estilo consistente e redu√ß√£o de lat√™ncia/custo.
- **Prompting:** Onde voc√™ deve gastar 90% do seu tempo inicial.

### [M√≥dulo 3: Tipos de Adapta√ß√£o](./03-adaptation-types)
- **Full Fine-Tuning:** Por que voc√™ quase nunca vai fazer isso.
- **PEFT / LoRA:** Como treinar modelos gigantes com pouco VRAM.
- **Instruction Tuning:** Ensinando o modelo a seguir ordens.
- **Likelihood Training (DPO/ORPO):** Ensinando o modelo o que voc√™ prefere.

### [M√≥dulo 4: Dados s√£o o Modelo](./04-data-prep)
- **A Verdade:** O modelo √© apenas um espelho dos seus dados.
- **Qualidade > Quantidade:** 100 exemplos perfeitos valem mais que 10.000 exemplos ruins.
- **Instruction Datasets:** Como formatar seus dados corretamente.

### [M√≥dulo 5: Avalia√ß√£o antes do Treino](./05-evaluation)
- **Regra:** Se voc√™ n√£o consegue medir, n√£o treine.
- **Baselines:** Como saber se o treino piorou o modelo (Catastrophic Forgetting).
- **LLM-as-a-Judge:** Usando GPT-4 para dar nota no seu Llama-3 finetunado.

### [M√≥dulo 6: Unsloth (Pr√°tico)](./06-unsloth)
- **A Ferramenta:** Por que Unsloth √© o padr√£o ouro hoje.
- **Efici√™ncia:** Treinando 2x mais r√°pido com 70% menos mem√≥ria.
- **Workflow:** Do notebook para o GGUF/LoRA Adapter.

### [M√≥dulo 7: Infra de Treino & Custo Real](./07-training-ops)
- **Hardware:** Quanto de VRAM voc√™ realmente precisa.
- **Spot Instances:** Economizando 70% na AWS/RunPod.
- **Custo Oculto:** O tempo de engenharia para limpar dados vs o custo de GPU.

### [M√≥dulo 8: Deploy & Infer√™ncia P√≥s-Treino](./08-deploy-adapters)
- **Adapters:** Como carregar LoRA adapters no vLLM sem duplicar o modelo base.
- **Merge:** Quando fundir os pesos (Mergekit) e quando carregar dinamicamente.
- **Drift:** Monitorando se o modelo "desaprendeu" coisas importantes.

### [M√≥dulo 9: Riscos & Manuten√ß√£o](./09-risks-maintenance)
- **Catastrophic Forgetting:** O modelo ficou √≥timo em SQL, mas esqueceu como falar ingl√™s.
- **Manuten√ß√£o:** Modelo treinado √© modelo "congelado". Como atualizar?

### [M√≥dulo 10: Enterprise & Gov](./10-enterprise-gov)
- **Compliance:** Quando o Fine-Tuning √© obrigat√≥rio por lei (On-premise total).
- **Privacidade:** Garantindo que dados sens√≠veis n√£o vazem.

---

## üõ†Ô∏è Stack de Treino (Padr√£o 2025)

| Componente | Escolha | Por qu√™? |
|:---|:---|:---|
| **Framework** | Unsloth | Velocidade e efici√™ncia de mem√≥ria imbat√≠veis. |
| **T√©cnica** | QLoRA (4-bit) | Permite treinar 70B em GPUs "baratas" (A6000/A100). |
| **Eval** | Ragas / LLM-as-Judge | Avalia√ß√£o escal√°vel antes de deploy. |
| **Dataset** | Hugging Face Datasets | Gerenciamento e versionamento de dados. |

## üß† Mudan√ßas Mentais Necess√°rias
- **Menos √© Mais:** Comece com 50 exemplos. Teste. Se melhorar, adicione mais.
- **Dados s√£o C√≥digo:** Trate seu dataset com o mesmo rigor que trata seu c√≥digo (versionamento, code review, linting).
- **Voc√™ provavelmente n√£o precisa de Fine-Tuning:** S√©rio. RAG + Few-Shot Prompting resolve 95% dos casos.

## üöÄ Como come√ßar
V√° para **[M√≥dulo 1: O que √© Fine-Tuning (Realmente)](./01-finetuning-concepts)**.


### 6. Inference optimization

Text generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.

* **Flash Attention**: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.
* **Key-value cache**: Understand the key-value cache and the improvements introduced in [Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA) and [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA).
* **Speculative decoding**: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation. EAGLE-3 is a particularly popular solution.

üìö **References**:
* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: Explain how to optimize inference on GPUs.
* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: Best practices for how to optimize LLM inference in production.
* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.
* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HF's version of speculative decoding. It's an interesting blog post about how it works with code to implement it.
* [EAGLE-3 paper](https://arxiv.org/abs/2503.01840?utm_source=chatgpt.com): Introduces EAGLE-3 and reports speedups up to 6.5√ó.
* [Speculators](https://github.com/vllm-project/speculators): Library made by vLLM for building, evaluating, and storing speculative decoding algorithms (e.g., EAGLE-3) for LLM inference.