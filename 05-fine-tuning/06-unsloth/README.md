# ü¶• M√≥dulo 6: Unsloth (Pr√°tico)

> **Goal:** O jeito certo de treinar em 2025.  
> **Status:** Essencial.

## 1. O que √© Unsloth?
√â uma biblioteca que reescreveu os kernels de backpropagation do Llama/Mistral manualmente.
- **Resultado:** Treina 2x mais r√°pido. Usa 70% menos mem√≥ria VRAM.
- **M√°gica:** Permite treino de contexto longo (8k, 16k) sem estourar a mem√≥ria.

## 2. O Workflow
1.  **Instala√ß√£o:** `pip install unsloth`
2.  **Load:** `FastLanguageModel.from_pretrained(..., load_in_4bit=True)`
3.  **PEFT:** `model = FastLanguageModel.get_peft_model(...)`
4.  **Train:** `Trainer.train()` (HuggingFace padr√£o).
5.  **Save:** `model.save_pretrained("lora_adapters")` e `model.save_pretrained_gguf(...)`.

## 3. Merging vs Adapter
- **Adapter Only (Recomendado):** Salve apenas os 100MB do LoRA. Carregue dinamicamente no vLLM.
- **Merged Model:** Funda os pesos (100MB + 70GB) em um novo arquivo de 70GB. Use apenas se a engine de infer√™ncia n√£o suportar LoRA.

## üß† Mental Model: "Unsloth √© o Turbo"
N√£o use `bitsandbytes` puro ou `transformers` puro se voc√™ puder usar Unsloth.
√â a mesma matem√°tica, s√≥ que otimizada. N√£o h√° perda de precis√£o.

## ‚è≠Ô∏è Pr√≥ximo Passo
Onde rodar isso?
V√° para **[M√≥dulo 7: Infra de Treino & Custo Real](../07-training-ops)**.
