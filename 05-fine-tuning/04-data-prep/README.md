# üìä M√≥dulo 4: Dados s√£o o Modelo

> **Goal:** Garbage In, Garbage Out muito r√°pido.  
> **Status:** Onde voc√™ vai gastar 80% do tempo.

## 1. O Paradoxo da Quantidade
Para treinar um LLM do zero: Precisa de Trilh√µes de tokens.
Para fazer Fine-Tuning: Precisa de **Centenas** de exemplos de *Extrema Qualidade*.

> Um dataset com 100 exemplos perfeitos (Quality > Quantity) supera um dataset com 10.000 exemplos sujos.

## 2. O Formato de Instru√ß√£o
O modelo n√£o aprende com texto solto. Ele aprende com pares.

```json
{
  "instruction": "Traduza para SQL do BigQuery.",
  "input": "Quantos usu√°rios ativos ontem?",
  "output": "SELECT count(*) FROM `users` WHERE last_active = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)"
}
```

## 3. Negative Examples (N√£o fa√ßa isso)
Se voc√™ quer que o modelo pare de ser verboso, n√£o corte apenas os exemplos verbosos.
Mostre exemplos onde ele *seria* verboso, e ensine a resposta curta.
Ou use DPO (Direct Preference Optimization) onde voc√™ explicitamente diz "Esta resposta curta vence esta resposta longa".

## 4. Onde conseguir dados?
1.  **Logs de Produ√ß√£o:** O melhor dataset s√£o as perguntas reais dos seus usu√°rios.
2.  **LLM Synthetic Data:** Use o GPT-4o para gerar exemplos de treino para o Llama-3-8B. (Destila√ß√£o de Modelo).
3.  **Human Review:** Pague humanos para corrigir os dados sint√©ticos.

## üß† Mental Model: "Livro Did√°tico"
Prepare seu dataset como se estivesse escrevendo um livro did√°tico para uma crian√ßa.
Exemplos claros. Sem ambiguidade. Formato consistente.

## ‚è≠Ô∏è Pr√≥ximo Passo
Como saber se o dataset √© bom *antes* de gastar dinheiro com GPU?
V√° para **[M√≥dulo 5: Avalia√ß√£o antes do Treino](../05-evaluation)**.
