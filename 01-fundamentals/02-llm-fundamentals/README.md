# ü§ñ M√≥dulo 06: Fundamentos de LLMs & GenAI

> **Goal:** Entender profundamente a mat√©ria‚Äëprima da nova computa√ß√£o.
>
> Este m√≥dulo n√£o ensina a *usar ferramentas*. Ele ensina a **pensar como um AI Engineer que trabalha com LLMs em produ√ß√£o**.

---

## üìå O que s√£o LLMs, de verdade

Large Language Models (LLMs) s√£o **modelos probabil√≠sticos autoregressivos** treinados para prever o pr√≥ximo token com base em um hist√≥rico de tokens.

Isso significa algo extremamente importante:

> ‚ùó O modelo **n√£o pensa, n√£o raciocina e n√£o entende**.
>
> Ele calcula probabilidades condicionais extremamente bem.

Tudo o que parece ‚Äúintelig√™ncia‚Äù emerge de escala:

* bilh√µes de par√¢metros
* trilh√µes de tokens
* arquiteturas Transformer

O papel do AI Engineer n√£o √© treinar isso.
√â **domar, controlar e orquestrar esse comportamento probabil√≠stico**.

---

## üß† A arquitetura mental correta

Antes de qualquer conceito t√©cnico, guarde isto:

> Um LLM √© um *motor estat√≠stico de linguagem com mem√≥ria tempor√°ria limitada*.

Ele:

* n√£o possui estado persistente
* n√£o lembra de intera√ß√µes passadas
* n√£o sabe o que √© verdade
* n√£o acessa bancos
* n√£o executa c√≥digo

Tudo isso **precisa ser constru√≠do ao redor dele**.

Essa √© a diferen√ßa entre:

* *prompt engineer* ‚ùå
* *AI engineer* ‚úÖ

---

# 1Ô∏è‚É£ Tokeniza√ß√£o ‚Äî A Unidade At√¥mica

LLMs n√£o trabalham com palavras.
Eles trabalham com **tokens**.

### O que √© um token?

Um token √© um fragmento estat√≠stico de texto.
Pode ser:

* uma palavra
* parte de uma palavra
* um n√∫mero
* um s√≠mbolo

Exemplos:

* "intelig√™ncia" ‚Üí pode virar 3 ou 4 tokens
* "9.11" pode gerar mais tokens que "9.9"

Isso acontece porque o tokenizer aprende padr√µes estat√≠sticos, n√£o sem√¢nticos.

---

### Por que isso importa?

Porque **tudo em LLM √© limitado por tokens**:

* Context window
* Custo
* Lat√™ncia
* Performance

Um modelo com contexto de 128k tokens **n√£o pensa melhor**.
Ele apenas consegue **ver mais texto ao mesmo tempo**.

---

### Input vs Output tokens

Isso √© fundamental em produ√ß√£o:

* **Input tokens** ‚Üí geralmente baratos
* **Output tokens** ‚Üí geralmente caros

Por isso:

* prompts longos custam
* respostas longas custam muito mais

AI Engineer bom otimiza:

* contexto
* tamanho de chunk
* quantidade de documentos
* verbosity da resposta

---

# 2Ô∏è‚É£ Context Window ‚Äî A Mem√≥ria Tempor√°ria

LLMs possuem apenas **mem√≥ria de curto prazo**.

Essa mem√≥ria √© o *context window*.

Tudo fora disso:

* n√£o existe
* n√£o √© lembrado
* n√£o influencia a resposta

Por isso:

* conversas longas degradam
* RAG existe
* agentes precisam resumir

O modelo n√£o ‚Äúlembra‚Äù.
Voc√™ precisa **reenviar o que importa**.

---

# 3Ô∏è‚É£ O Ciclo de Vida do Prompt

Prompt engineering n√£o √© escrever texto bonito.

√â **engenharia de contexto**.

Um prompt completo possui tr√™s camadas:

---

## 3.1 System Prompt

O system prompt define:

* papel do modelo
* comportamento
* limites
* regras

Ele atua como uma **camada constitucional**.

Exemplo conceitual:

* voc√™ √© um auditor
* responda apenas com base no contexto
* nunca invente informa√ß√µes

Em produ√ß√£o, esse prompt deve ser:

* versionado
* testado
* tratado como c√≥digo

---

## 3.2 Few‚ÄëShot Learning

Modelos aprendem melhor por exemplo do que por instru√ß√£o.

Few‚Äëshot √© quando voc√™ mostra:

> "Quando a entrada for assim, a sa√≠da esperada √© assim."

Isso √© extremamente poderoso para:

* formatos
* classifica√ß√£o
* padroniza√ß√£o
* tomada de decis√£o

LLMs copiam padr√µes estat√≠sticos.
Few‚Äëshot explora isso diretamente.

---

## 3.3 User Prompt

√â a parte din√¢mica.

Nunca deve conter regras cr√≠ticas.
Nunca deve definir comportamento.

Tudo que √© importante deve estar no system prompt.

---

# 4Ô∏è‚É£ Temperatura, Top‚ÄëP e Amostragem

Esses par√¢metros controlam **aleatoriedade**.

* Temperature baixa ‚Üí respostas determin√≠sticas
* Temperature alta ‚Üí criatividade

Em produ√ß√£o:

* temperatura costuma ser baixa (0‚Äì0.3)
* previsibilidade √© mais importante que criatividade

LLM corporativo ‚â† chatbot criativo.

---

# 5Ô∏è‚É£ Structured Outputs ‚Äî Probabil√≠stico ‚Üí Determin√≠stico

LLMs s√£o probabil√≠sticos.
Produ√ß√£o exige determinismo.

A solu√ß√£o √© **Structured Output**.

Nunca confie em:

* markdown
* regex
* parsing textual

Sempre use:

* JSON Schema
* response_format
* tool calling

Isso transforma o LLM em um **gerador de objetos v√°lidos**.

Esse √© um dos pilares mais importantes da engenharia moderna com LLMs.

---

# 6Ô∏è‚É£ Tool Calling (Function Calling)

Aqui ocorre a virada de chave.

O LLM n√£o executa a√ß√µes.

Mas ele pode:

* decidir qual a√ß√£o executar
* estruturar os argumentos
* delegar execu√ß√£o

Fluxo real:

1. Usu√°rio pergunta algo
2. LLM decide chamar uma fun√ß√£o
3. Retorna JSON estruturado
4. Seu c√≥digo executa
5. Resultado volta ao LLM

Isso cria **agentes reais**.

---

### O LLM n√£o age. Ele orquestra.

Quem executa √©:

* Python
* APIs
* bancos
* servi√ßos

O LLM apenas escolhe.

Esse princ√≠pio √© cr√≠tico.

---

# 7Ô∏è‚É£ LLM ‚â† Agente

Um erro comum:

> "Estou usando agentes porque uso LLM."

Errado.

Um agente possui:

* objetivo
* ferramentas
* estado
* loop de decis√£o

O LLM √© apenas o c√©rebro probabil√≠stico.

Frameworks como:

* LangGraph
* CrewAI
* AutoGen

existem para construir o **loop de controle**.

---

# 8Ô∏è‚É£ Multimodalidade

LLMs modernos operam com:

* texto
* imagem
* √°udio
* v√≠deo

Tudo vira embedding.

Isso permite:

* an√°lise de documentos escaneados
* interpreta√ß√£o de imagens
* agentes visuais
* pipelines multimodais

Pensar s√≥ em texto hoje √© limitar brutalmente o potencial do sistema.

---

# 9Ô∏è‚É£ Fine‚ÄëTuning vs RAG vs Prompt

Essa decis√£o separa amadores de engenheiros.

### Prompt Engineering

* r√°pido
* barato
* flex√≠vel

Ideal quando:

* regras mudam
* contexto √© pequeno

---

### RAG

* injeta conhecimento externo
* mant√©m modelo gen√©rico
* altamente escal√°vel

Ideal quando:

* dados s√£o privados
* documentos mudam
* auditoria √© necess√°ria

---

### Fine‚ÄëTuning

* caro
* r√≠gido
* dif√≠cil de versionar

S√≥ vale quando:

* padr√£o √© extremamente repetitivo
* lat√™ncia precisa ser m√≠nima
* prompt n√£o resolve

AI Engineer experiente evita fine‚Äëtuning prematuro.

---

# üîü Alucina√ß√£o ‚Äî N√£o √© bug, √© caracter√≠stica

O modelo sempre tenta responder.

Se n√£o sabe, ele:

* completa estatisticamente

Isso gera hallucination.

Solu√ß√µes reais:

* grounding
* RAG
* cita√ß√µes
* valida√ß√£o
* confian√ßa m√≠nima

Nunca confie apenas no modelo.

---

# üß† O papel real do AI Engineer

O trabalho n√£o √© fazer o modelo falar.

√â:

* controlar contexto
* limitar comportamento
* estruturar respostas
* validar sa√≠das
* medir qualidade
* reduzir custo
* garantir confiabilidade

O LLM √© s√≥ um componente.

O sistema √© o produto.

---

# ‚úÖ Conclus√£o

LLMs s√£o a nova CPU.

Mas uma CPU sozinha n√£o resolve nada.

O verdadeiro poder est√° em:

* arquitetura
* dados
* controle
* engenharia

Dominar esses fundamentos √© o que separa:

* quem faz demo
* de quem constr√≥i sistemas de IA reais

---

‚è≠Ô∏è **Pr√≥ximo passo:**
Sem mem√≥ria externa, o LLM continua cego.

V√° para **M√≥dulo 08 ‚Äî RAG (Retrieval‚ÄëAugmented Generation)**.
