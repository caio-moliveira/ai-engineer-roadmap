## ðŸ“š 10 *Papers* que todo AI Engineer deve ler (com links)

1. **Attention Is All You Need** (Vaswani et al., 2017)
   Arquitetura *Transformer*, base de todos os LLMs modernos. ([Wikipedia][2])
   ðŸ“„ [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

2. **Language Models are Few-Shot Learners** (Brown et al., 2020)
   GPT-3 â€” mostra a capacidade de *in-context learning* e *few-shot learning*. ([Medium][1])
   ðŸ“„ [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)

3. **Training Language Models to Follow Instructions with Human Feedback** (InstructGPT / RLHF)
   Introduz *Reinforcement Learning from Human Feedback* para alinhamento de LLMs. ([LinkedIn][3])
   ðŸ“„ [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)

4. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** (RAG)
   Combina *retrieval* e *generation* para respostas mais precisas e atualizadas. ([robertodiasduarte.com.br][4])
   ðŸ“„ [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)

5. **LoRA: Low-Rank Adaptation of Large Language Models**
   TÃ©cnica para *fine-tuning* eficiente e barato de LLMs. ([LinkedIn][5])
   ðŸ“„ [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)

6. **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity**
   Mostra como escalar LLMs com *Mixture of Experts* (MoE). ([LinkedIn][6])
   ðŸ“„ [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)

7. **LLM.int8(): 8-Bit Matrix Multiplication for Transformers at Scale**
   QuantizaÃ§Ã£o eficiente para rodar LLMs com menor custo de memÃ³ria/computaÃ§Ã£o. ([LinkedIn][6])
   ðŸ“„ [https://arxiv.org/abs/2309.04643](https://arxiv.org/abs/2309.04643)

8. **DistilBERT: A Distilled Version of BERT** (Sanh et al., 2019)
   Demonstrou *distillation* como forma de criar modelos menores e rÃ¡pidos. ([LinkedIn][6])
   ðŸ“„ [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)

9. **Chain of Thought Prompting** (Wei et al., 2022)
   Explora como decompor lÃ³gica do modelo para melhor raciocÃ­nio. ([LinkedIn][5])
   ðŸ“„ [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)

10. **Scaling Laws for Neural Language Models** (Kaplan et al., 2020)
    Mostra relaÃ§Ãµes previsÃ­veis de performance com tamanho de modelo e dados. ([LinkedIn][5])
    ðŸ“„ [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)

---

## ðŸ’¡ Como usar essa lista na sua documentaÃ§Ã£o

### ðŸŽ¯ Para cada paper:

```markdown
### 1. Attention Is All You Need
**Resumo:** Introduziu a arquitetura Transformer que revolucionou NLP e tornou viÃ¡veis modelos de linguagem em grande escala.  
**Por que ler:** Base para entender como LLMs funcionam por dentro.  
**Link:** https://arxiv.org/abs/1706.03762
```

### ðŸ“Œ Sugerido para capÃ­tulos:

| TÃ³pico curricular                       | Papers recomendados                                                   |
| --------------------------------------- | --------------------------------------------------------------------- |
| **Fundamentos de LLMs**                 | *Attention Is All You Need*, *Language Models are Few-Shot Learners*  |
| **Alinhamento e comportamento**         | *Training Language Models to Follow Instructions with Human Feedback* |
| **RAG e sistemas conectados Ã  memÃ³ria** | *Retrieval-Augmented Generation*                                      |
| **EficiÃªncia & produÃ§Ã£o**               | *LoRA*, *LLM.int8()*, *DistilBERT*, *Scaling Laws*                    |
| **RaciocÃ­nio & prompting avanÃ§ado**     | *Chain of Thought Prompting*                                          |
| **Escala& arquitetura**                 | *Switch Transformers*                                                 |

---

## ðŸ§  Por que este conjunto importa para um AI Engineer

Esses papers **nÃ£o sÃ£o apenas teoria** â€” eles explicam:

* **Como os modelos sÃ£o construÃ­dos (Transformers).**
* **Como eles aprendem com pouco contexto (GPT-3).**
* **Como alinhÃ¡-los a intenÃ§Ãµes humanas (RLHF).**
* **Como conectÃ¡-los a conhecimentos externos (RAG).**
* **Como otimizar e operar modelos em produÃ§Ã£o (LoRA, quantizaÃ§Ã£o).**
* **Como decompor tarefas complexas (Chain of Thought).**

Juntos, eles formam uma base sÃ³lida para quem quer trabalhar **com sistemas de IA em produÃ§Ã£o**, nÃ£o apenas treinar modelos em notebooks.

