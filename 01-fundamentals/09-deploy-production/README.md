# üö¢ M√≥dulo 09: Deploy, Infra e Produ√ß√£o

> **Goal:** Ensinar como colocar uma API de IA em produ√ß√£o de forma profissional, previs√≠vel e sustent√°vel.
>
> Este m√≥dulo √© escrito no formato de **documenta√ß√£o t√©cnica**, n√£o como checklist conceitual.
> O foco √© **deploy de backend de IA** ‚Äî APIs que executam agentes, RAGs e workflows com LLMs.

---

## üß† Introdu√ß√£o ‚Äî O que significa "deploy" em sistemas de IA

Em aplica√ß√µes tradicionais, deploy significa:

* subir um backend
* expor endpoints
* escalar requisi√ß√µes

Em sistemas de IA, deploy significa algo muito mais complexo:

* garantir reprodutibilidade do ambiente
* controlar depend√™ncias pesadas
* proteger chaves sens√≠veis
* evitar custo descontrolado
* garantir comportamento consistente

Uma API de IA n√£o pode:

* inicializar coisas no meio da request
* depender de estado local
* carregar modelos de forma pregui√ßosa

Tudo deve estar **deterministicamente pronto no startup do container**.

---

## üì¶ Arquitetura base de uma API de IA

Antes de falar de Docker, √© importante entender o que estamos empacotando.

Uma API de IA t√≠pica possui:

```
app/
 ‚îú‚îÄ‚îÄ main.py            # FastAPI entrypoint
 ‚îú‚îÄ‚îÄ api/               # rotas HTTP
 ‚îú‚îÄ‚îÄ core/              # configura√ß√µes e settings
 ‚îú‚îÄ‚îÄ llm/               # clients de modelos
 ‚îú‚îÄ‚îÄ rag/               # pipelines de retrieval
 ‚îú‚îÄ‚îÄ agents/            # fluxos agentic
 ‚îú‚îÄ‚îÄ observability/     # tracing, logs
 ‚îî‚îÄ‚îÄ services/          # regras de neg√≥cio
```

O Docker n√£o resolve arquitetura ruim.
Ele apenas empacota.

---

# 1Ô∏è‚É£ Docker ‚Äî por que ele √© obrigat√≥rio em IA

Docker garante que:

* o mesmo c√≥digo rode igual em qualquer ambiente
* as mesmas vers√µes de libs sejam usadas
* o runtime do modelo seja previs√≠vel

Sem Docker, o comportamento do LLM pode variar apenas por diferen√ßa de depend√™ncias.

---

## 1.1 Diferen√ßa pr√°tica: Web API vs API de IA

| Aspecto      | Web comum  | IA         |
| ------------ | ---------- | ---------- |
| Depend√™ncias | leves      | pesadas    |
| Startup      | r√°pido     | mais lento |
| Mem√≥ria      | baixa      | alta       |
| Custo        | previs√≠vel | vari√°vel   |

Isso muda completamente o Dockerfile.

---

# 2Ô∏è‚É£ Dockerfile explicado linha por linha

A seguir est√° um **Dockerfile realista para uma API FastAPI de IA**.

```dockerfile
FROM python:3.11-slim AS base

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# depend√™ncias do sistema
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# copiar apenas depend√™ncias primeiro
COPY pyproject.toml uv.lock ./

RUN pip install --no-cache-dir uv \
    && uv pip install --system

# agora copia o c√≥digo
COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Por que essa ordem importa?

Docker cria camadas.

Se voc√™ copiar o c√≥digo antes das depend√™ncias:

* qualquer altera√ß√£o invalida o cache
* tudo √© reinstalado

Separando depend√™ncias:

* mudan√ßas de c√≥digo n√£o reinstalam libs
* build fica muito mais r√°pido

---

# 3Ô∏è‚É£ Multistage build (quando necess√°rio)

Em projetos de IA mais pesados (torch, sentence-transformers, vLLM), usamos multistage build.

Exemplo conceitual:

```dockerfile
FROM python:3.11 AS builder
WORKDIR /build
COPY pyproject.toml uv.lock ./
RUN pip install uv && uv pip install --system

FROM python:3.11-slim
WORKDIR /app
COPY --from=builder /usr/local /usr/local
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

O est√°gio final fica muito menor.

---

# 4Ô∏è‚É£ Vari√°veis de ambiente (configura√ß√£o correta)

Nada sens√≠vel pode existir no c√≥digo.

Nunca:

```python
OPENAI_API_KEY = "sk-xxxx"
```

Tudo deve vir do ambiente.

---

## 4.1 Pydantic Settings (fail fast)

Exemplo real:

```python
class Settings(BaseSettings):
    openai_api_key: str
    environment: str

    class Config:
        env_file = ".env"
```

Se a vari√°vel n√£o existir:

* a aplica√ß√£o falha no startup
* o container n√£o sobe

Isso evita falhas silenciosas.

---

# 5Ô∏è‚É£ Startup lifecycle da API

Uma API de IA deve inicializar tudo no startup:

```python
@app.on_event("startup")
async def startup():
    load_llm_clients()
    connect_vector_db()
    warmup_embeddings()
```

Nada cr√≠tico deve ser criado durante uma request.

Requests devem apenas:

* executar l√≥gica
* usar recursos j√° prontos

---

# 6Ô∏è‚É£ Streaming de resposta (SSE)

LLMs s√£o lentos.

Se voc√™ esperar o modelo terminar para responder:

* o usu√°rio acha que travou

Com streaming:

```python
return EventSourceResponse(generator())
```

O usu√°rio come√ßa a ver tokens imediatamente.

Isso melhora drasticamente a experi√™ncia.

---

# 7Ô∏è‚É£ Prote√ß√£o de custo

Sem prote√ß√£o, uma √∫nica rota pode gerar milhares de tokens.

Boas pr√°ticas:

* limitar tamanho do prompt
* limitar hist√≥rico
* timeout por request
* rate limiting

Isso n√£o √© otimiza√ß√£o.
√â sobreviv√™ncia financeira.

---

# 8Ô∏è‚É£ Deploy mental model

Deploy n√£o √© o final.

√â o in√≠cio da vida do sistema.

Depois do deploy v√™m:

* m√©tricas
* tracing
* avalia√ß√£o
* otimiza√ß√£o

Por isso este m√≥dulo se conecta diretamente com observabilidade.

---

# üèÅ Conclus√£o

Colocar IA em produ√ß√£o exige:

* disciplina de engenharia
* arquitetura limpa
* containers bem constru√≠dos
* controle de ambiente

N√£o existe sistema de IA confi√°vel sem deploy profissional.

---

‚è≠Ô∏è **Pr√≥xima Etapa:** Construir um sistema RAG real, do zero.
Nos vemos no **Bloco 2: Sistemas RAG**.
