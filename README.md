<div align="center">
  <img src="./assets/jornada.png" alt="Jornada de Dados" width="200"/>

# **Trilha Completa: Engenharia de IA**

### ConstruÃ§Ã£o profissional de sistemas de IA, RAGs e agentes em produÃ§Ã£o

**FormaÃ§Ã£o prÃ¡tica focada em arquitetura, orquestraÃ§Ã£o, observabilidade e deploy de aplicaÃ§Ãµes de IA modernas**

</div>

---

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Python: 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/)

[**Site Oficial**](https://suajornadadedados.com.br/) â€¢ [**Comunidade**](https://suajornadadedados.com.br/) â€¢ [**DocumentaÃ§Ã£o**](https://suajornadadedados.com.br/)
</div>

---

## ğŸš€ O Manifesto do Engenheiro de IA (EdiÃ§Ã£o 2026)

O mercado de "Power Users" de chat saturou. Em 2026, a barreira de entrada nÃ£o Ã© mais saber o que Ã© um prompt, mas sim como garantir **determinismo, seguranÃ§a e custo-eficiÃªncia** em sistemas nÃ£o-determinÃ­sticos.

### ğŸ¯ Onde vocÃª se posiciona?
Diferente do **Cientista de Dados** (focado em *Training & Fine-tuning*) e do **ML Engineer** (focado em *Infrastructure & Serving*), o **AI Engineer** Ã© o engenheiro de software especializado na composiÃ§Ã£o de Modelos de FundaÃ§Ã£o.

> **"Nossa missÃ£o nÃ£o Ã© criar inteligÃªncia bruta, mas arquitetar o contexto necessÃ¡rio para que ela seja Ãºtil."**

### ğŸ’ Pilares da Engenharia de IA Moderna

Para mover o ponteiro em projetos reais, atacamos os trÃªs pilares que separam demos de produtos:

1.  **Fidelidade (Grounding):** ImplementaÃ§Ã£o de RAG (Retrieval-Augmented Generation) multicamadas para eliminar alucinaÃ§Ãµes.
2.  **Autonomia (Agency):** EvoluÃ§Ã£o de fluxos lineares para grafos cÃ­clicos com **LangGraph**, permitindo raciocÃ­nio complexo e correÃ§Ã£o de erros em tempo real.
3.  **LLMOps & Observabilidade:** Se vocÃª nÃ£o mede, vocÃª nÃ£o gerencia. Utilizamos **Langfuse** e **Arize Phoenix** para rastreabilidade total de tokens, latÃªncia e custo.

---

## ğŸ› ï¸ O Tech Stack do Especialista
NÃ£o ensinamos apenas ferramentas; ensinamos os padrÃµes de design de software aplicados Ã  IA:

* **Linguagem & Base:** Python Pro (AsyncIO), Pydantic (ValidaÃ§Ã£o de Dados) e Docker.
* **Vector Architecture:** Qdrant, Pinecone e ChromaDB para busca semÃ¢ntica e hÃ­brida.
* **OrquestraÃ§Ã£o de Estado:** LangChain e LangGraph para fluxos de agentes com memÃ³ria persistente.
* **Engenharia de Prompt:** Chain-of-Thought, Few-shot prompting e tÃ©cnicas de compressÃ£o de contexto.

---

## ğŸ“š A Trilha de FormaÃ§Ã£o
<div align="center">
<img src="./assets/roadmap.png" alt="Roadmap" width="1000"/>
</div>


### [ğŸ”¹ Bloco 1: Fundamentos Reais](./01-fundamentals)

Este bloco define a base conceitual e tÃ©cnica necessÃ¡ria para construir sistemas modernos de InteligÃªncia Artificial em produÃ§Ã£o.

NÃ£o Ã© um bloco sobre sintaxe, bibliotecas isoladas ou experimentaÃ§Ã£o em notebooks.
Ã‰ sobre entender como desenvolver software quando o componente central do sistema Ã© **probabilÃ­stico, assÃ­ncrono e com custo operacional variÃ¡vel**.

A engenharia de sistemas baseados em Large Language Models difere fundamentalmente do desenvolvimento tradicional porque:

* a saÃ­da nÃ£o Ã© determinÃ­stica
* erros podem parecer respostas vÃ¡lidas
* latÃªncia depende de inferÃªncia externa
* custo depende diretamente de tokens consumidos

A prÃ³pria OpenAI enfatiza que o trabalho real com IA nÃ£o Ã© treinar modelos, mas construir sistemas ao redor deles:
[https://cookbook.openai.com/](https://cookbook.openai.com/)

Arquiteturas corporativas modernas seguem o mesmo princÃ­pio, tratando LLMs como apenas um componente dentro de pipelines maiores (AWS Generative AI Reference Architecture):
[https://aws.amazon.com/architecture/generative-ai/](https://aws.amazon.com/architecture/generative-ai/)

Este bloco existe para estabelecer os fundamentos que tornam possÃ­vel construir sistemas confiÃ¡veis nessas condiÃ§Ãµes.

---

### Estrutura do Bloco 

#### [MÃ³dulo 01: A ProfissÃ£o de AI Engineer & Mercado](./01-fundamentals/01-ai-engineer-profession)

Este mÃ³dulo define o papel profissional do AI Engineer no ecossistema moderno de software.

Ele aborda:

* a diferenÃ§a entre AI Engineer, ML Engineer e Backend Engineer
* as expectativas reais do mercado corporativo
* o perfil tÃ©cnico necessÃ¡rio para operar sistemas de IA

A distinÃ§Ã£o entre construir modelos e construir sistemas Ã© central para a indÃºstria atual. DocumentaÃ§Ã£o de plataformas corporativas como Azure AI Architecture Guide enfatiza explicitamente a necessidade de integraÃ§Ã£o, observabilidade e governanÃ§a como partes essenciais do desenvolvimento:
[https://learn.microsoft.com/en-us/azure/architecture/ai-ml/](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/)

A noÃ§Ã£o de engenharia orientada a produto tambÃ©m aparece em relatÃ³rios de mercado como McKinsey AI Adoption Report:
[https://www.mckinsey.com/capabilities/quantumblack/our-insights](https://www.mckinsey.com/capabilities/quantumblack/our-insights)

O objetivo deste mÃ³dulo Ã© alinhar o aluno com a realidade operacional da profissÃ£o antes de qualquer ferramenta.

---

#### [MÃ³dulo 02: Fundamentos de LLMs & GenAI](./01-fundamentals/02-llm-fundamentals)

Este mÃ³dulo apresenta os princÃ­pios matemÃ¡ticos e operacionais dos Large Language Models.

Ele cobre:

* tokenizaÃ§Ã£o e representaÃ§Ã£o textual
* janelas de contexto
* parÃ¢metros de geraÃ§Ã£o como temperature e sampling
* tÃ©cnicas modernas de prompt engineering
* tool calling e execuÃ§Ã£o externa

A arquitetura dominante dos LLMs modernos Ã© baseada no Transformer, introduzido em:

Attention Is All You Need
[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

A capacidade de aprendizado por exemplos no contexto foi demonstrada no paper do GPT-3:

Language Models are Few-Shot Learners
[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)

O uso de raciocÃ­nio explÃ­cito em prompts Ã© discutido em:

Chain-of-Thought Prompting
[https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)

E a integraÃ§Ã£o entre raciocÃ­nio e aÃ§Ã£o via ferramentas aparece em:

ReAct: Synergizing Reasoning and Acting
[https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)

Este mÃ³dulo estabelece o entendimento necessÃ¡rio para tratar LLMs como componentes de engenharia, nÃ£o interfaces conversacionais.

---

#### [MÃ³dulo 03: Python Moderno para AI Engineers](./01-fundamentals/03-python-for-ai)

Este mÃ³dulo estabelece o ambiente de engenharia necessÃ¡rio para sistemas de IA em produÃ§Ã£o.

Ele cobre:

* gerenciamento moderno de dependÃªncias
* tipagem estÃ¡tica
* arquitetura modular de projetos
* padrÃµes de organizaÃ§Ã£o de cÃ³digo

Sistemas de IA possuem dependÃªncias pesadas e altamente sensÃ­veis (CUDA, Torch, bibliotecas nativas).
Por esse motivo, a reprodutibilidade do ambiente Ã© considerada requisito operacional bÃ¡sico em engenharia de ML moderna (ver MLOps Principles, Google Cloud):
[https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)

O mÃ³dulo tambÃ©m introduz bibliotecas fundamentais para aplicaÃ§Ãµes assÃ­ncronas e orientadas a serviÃ§os, alinhadas com prÃ¡ticas modernas de backend Python.

---

#### [MÃ³dulo 04: APIs & Backend com FastAPI](./01-fundamentals/04-fastapi-backend)

Este mÃ³dulo cobre a construÃ§Ã£o da camada de serviÃ§o responsÃ¡vel por expor sistemas de IA.

Ele aborda:

* programaÃ§Ã£o assÃ­ncrona com async/await
* injeÃ§Ã£o de dependÃªncia
* definiÃ§Ã£o automÃ¡tica de contratos OpenAPI
* tratamento de concorrÃªncia

AplicaÃ§Ãµes de IA sÃ£o tipicamente I/O bound, dependendo de chamadas externas para inferÃªncia.
Frameworks assÃ­ncronos sÃ£o recomendados para esse cenÃ¡rio, conforme documentado no prÃ³prio FastAPI:

[https://fastapi.tiangolo.com/async/](https://fastapi.tiangolo.com/async/)

A geraÃ§Ã£o automÃ¡tica de documentaÃ§Ã£o via OpenAPI tambÃ©m segue padrÃµes amplamente adotados na indÃºstria de APIs:
[https://swagger.io/specification/](https://swagger.io/specification/)

Este mÃ³dulo ensina como transformar pipelines de IA em serviÃ§os acessÃ­veis e escalÃ¡veis.

---

#### [MÃ³dulo 05: Modelagem e Contratos de Dados](./01-fundamentals/05-data-modeling)

Este mÃ³dulo aborda a camada crÃ­tica de confiabilidade de sistemas baseados em LLM.

Ele cobre:

* JSON Schema
* serializaÃ§Ã£o estruturada
* validaÃ§Ã£o automÃ¡tica
* definiÃ§Ã£o de contratos rÃ­gidos para entrada e saÃ­da

Modelos de linguagem nÃ£o garantem consistÃªncia estrutural na saÃ­da.
Por isso, a validaÃ§Ã£o de schemas Ã© considerada prÃ¡tica essencial na documentaÃ§Ã£o oficial de Structured Outputs da OpenAI:

[https://platform.openai.com/docs/guides/structured-outputs](https://platform.openai.com/docs/guides/structured-outputs)

O mÃ³dulo utiliza Pydantic v2 para formalizar contratos de dados e garantir integridade em pipelines probabilÃ­sticos.

---

#### [MÃ³dulo 06: Bancos de Dados (SQL + Vetorial)](./01-fundamentals/06-databases)

Este mÃ³dulo introduz a arquitetura de armazenamento hÃ­brido necessÃ¡ria para aplicaÃ§Ãµes modernas de IA.

Ele cobre:

* integraÃ§Ã£o entre banco relacional e banco vetorial
* embeddings e representaÃ§Ã£o semÃ¢ntica
* distÃ¢ncia de cosseno
* filtragem por metadados

A estratÃ©gia de Retrieval-Augmented Generation foi formalizada em:

Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
[https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)

A busca vetorial em larga escala normalmente utiliza algoritmos Approximate Nearest Neighbor como HNSW, descritos em:

Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs
[https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)

Este mÃ³dulo estabelece a base de armazenamento necessÃ¡ria para construir sistemas conectados a dados proprietÃ¡rios.

---

## ReferÃªncias Complementares

Run an LLM locally with LM Studio [https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)
Prompt Engineering Guide[https://www.promptingguide.ai/](https://www.promptingguide.ai/)
Outlines Quickstart[https://dottxt-ai.github.io/outlines/latest/quickstart/](https://dottxt-ai.github.io/outlines/latest/quickstart/)
LMQL Overview[https://lmql.ai/docs/language/overview.html](https://lmql.ai/docs/language/overview.html)


### [ğŸ”¹ Bloco 2: Sistemas RAG](./02-rag)
Este nÃ£o Ã© apenas "Chat with PDF". RAG em produÃ§Ã£o exige estratÃ©gias de Chunking, Reranking e AvaliaÃ§Ã£o.

#### [MÃ³dulo 01: Fundamentos de RAG e Modelos Mentais](./02-rag/01-rag-fundamentals)
- **DefiniÃ§Ã£o:** RAG = Busca (Retrieval) + GeraÃ§Ã£o (Generation).
- **Por que RAG?** Superando alucinaÃ§Ãµes e data de corte (knowledge cutoff).
- **Arquitetura PadrÃ£o:** Ingestion -> Store -> Retrieve -> Generate.

#### [MÃ³dulo 02: IngestÃ£o de Dados e Pipelines](./02-rag/02-ingestion-pipeline)
- **ETL para IA:** Extrair texto limpo de PDFs, HTML e Markdown.
- **Chunking:** EstratÃ©gias (Fixed-size, Recursive, Semantic) e seus impactos.
- **Metadados:** Por que metadados sÃ£o mais importantes que o texto em si.

#### [MÃ³dulo 03: Embeddings (VisÃ£o Moderna)](./02-rag/03-embeddings)
- **Conceito:** Transformando texto em vetores numÃ©ricos.
- **Modelos:** OpenAI vs Open Source (bge-m3, e5).
- **Multilingual:** Lidando com portuguÃªs e inglÃªs misturados.

#### [MÃ³dulo 04: Vetor Databases (Vector DBs)](./02-rag/04-vector-dbs)
- **OpÃ§Ãµes:** Qdrant (Rust/Performance) vs pgvector (Simplicidade/Postgres).
- **IndexaÃ§Ã£o:** HNSW explicado para humanos.
- **Tradeoffs:** MemÃ³ria vs Disco vs Velocidade.

#### [MÃ³dulo 05: EstratÃ©gias de Retrieval (CrÃ­tico)](./02-rag/05-retrieval-strategies)
- **Hybrid Search:** Misturando busca semÃ¢ntica (Vetores) com busca exata (BM25/Keywords).
- **Reranking:** O segredo para dobrar a precisÃ£o. (Cohere Rerank / Cross Encoders).
- **Query Expansion:** Melhorando a pergunta do usuÃ¡rio antes de buscar.

#### [MÃ³dulo 06: LangChain v1 (LCEL)](./02-rag/06-langchain-v1)
- **Modern LangChain:** EsqueÃ§a `RetrievalQAChain`. Use LCEL (LangChain Expression Language).
- **Composabilidade:** Pipelines declarativos e transparentes.
- **Runnables:** O protocolo padrÃ£o para invocar cadeias.

#### [MÃ³dulo 07: LangGraph (OrquestraÃ§Ã£o RAG)](./02-rag/07-langgraph)
- **Loops:** Quando a busca linear falha, precisamos de loops (agentes).
- **Corrective RAG:** Se a busca for ruim, pesquise na web. (Flow condicional).
- **Estado:** Mantendo memÃ³ria durante a execuÃ§Ã£o do grafo.

#### [MÃ³dulo 08: LlamaIndex](./02-rag/08-llamaindex)
- **Foco em Dados:** Quando usar LlamaIndex em vez de LangChain.
- **Advanced Indexing:** Hierarchical Indices, Document Summary Index.
- **Query Engine:** AbstraÃ§Ãµes poderosas para dados complexos.

#### [MÃ³dulo 09: AvaliaÃ§Ã£o e Observabilidade](./02-rag/09-evaluation)
- **Ragas:** Framework de avaliaÃ§Ã£o automÃ¡tica (Faithfulness, Answer Relevancy).
- **Tracing:** Visualizando cada passo com Langsmith/Langfuse.
- **Golden Datasets:** Criando um conjunto de testes confiÃ¡vel.

#### [MÃ³dulo 10: RAG em ProduÃ§Ã£o](./02-rag/10-rag-production)
- **OtimizaÃ§Ã£o:** Cache SemÃ¢ntico, Streaming, LatÃªncia.
- **SeguranÃ§a:** Prompt Injection em RAG.
- **Custos:** Estimando tokens de input/output em escala.

---

### ğŸ› ï¸ Stack RAG (PadrÃ£o 2025)

| Componente | Escolha | Por quÃª? |
|:---|:---|:---|
| **OrquestraÃ§Ã£o** | LangChain / LangGraph | Flexibilidade e ecossistema. |
| **Vector DB** | Qdrant / pgvector | Performance e facilidade de uso. |
| **Embeddings** | OpenAI (text-3) / Cohere | Qualidade e facilidade. |
| **LLM** | GPT-4o / Claude 3.5 Sonnet | RaciocÃ­nio superior para sÃ­ntese. |
| **Eval** | Ragas | PadrÃ£o de mercado para mÃ©tricas RAG. |

### ğŸ§  MudanÃ§as Mentais NecessÃ¡rias
- **Busca SemÃ¢ntica nÃ£o Ã© MÃ¡gica:** Ela falha em "termos exatos" (IDs, SKUs). Por isso usamos Hybrid Search.
- **Garbage In, Garbage Out:** Se seu chunking cortar a frase no meio, o LLM nÃ£o vai entender. Invista tempo na IngestÃ£o.

### ğŸ“š ReferÃªncias Recomendadas
* [LangChain - Text splitters](https://python.langchain.com/docs/how_to/#text-splitters): Lista de diferentes divididores de texto no LangChain.
* [Sentence Transformers library](https://www.sbert.net/): Biblioteca popular para modelos de embedding.
* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard): Leaderboard para modelos de embedding.
* [The Top 7 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases) por Moez Ali: ComparaÃ§Ã£o dos melhores bancos de dados vetoriais.
* [Llamaindex - High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html): Conceitos principais de RAG.
* [Model Context Protocol](https://modelcontextprotocol.io/introduction): IntroduÃ§Ã£o ao MCP.
* [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/): VisÃ£o geral de RAG.
* [LangChain - Q&A with RAG](https://python.langchain.com/docs/tutorials/rag/): Tutorial passo-a-passo de RAG.
* [LangChain - Query Construction](https://blog.langchain.dev/query-construction/): Tipos de construÃ§Ã£o de consulta.
* [LangChain - SQL](https://python.langchain.com/docs/tutorials/sql_qa/): Interagindo com SQL via LLMs.
* [Pinecone - LLM agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/): IntroduÃ§Ã£o a agentes e ferramentas.
* [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) por Lilian Weng: Artigo teÃ³rico sobre agentes.
* [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task): Guia de DSPy.

---

### [ğŸ”¹ Bloco 3: Agentes de IA](./03-ai-agents)
Agentes nÃ£o sÃ£o mÃ¡gicos. SÃ£o **Sistemas de Software** com **Autonomia Controlada**.

#### [MÃ³dulo 01: O que sÃ£o Agentes de IA (Realmente)](./03-ai-agents/01-agent-definitions)
- **DefiniÃ§Ã£o:** A diferenÃ§a entre um Workflow (RAG) e um Agente (Loop de RaciocÃ­nio).
- **Realidade:** Por que a maioria das demos de agentes falha miseravelmente em produÃ§Ã£o.
- **Spectrum:** De "Router Simples" a "Multi-Agent Swarm".

#### [MÃ³dulo 02: Arquiteturas de Agentes](./03-ai-agents/02-agent-architectures)
- **Patterns:** ReAct, Plan-and-Solve, Reflection.
- **Design:** Agentes Reativos vs Agentes Deliberativos.
- **Tradeoffs:** Quando usar um grafo (LangGraph) vs uma chain linear.

#### [MÃ³dulo 03: LangChain v1 para Agentes](./03-ai-agents/03-langchain-agents)
- **Tool Calling:** Como definir ferramentas com schemas Pydantic rigorosos.
- **Structured Output:** ForÃ§ando o agente a responder JSON validado, nÃ£o texto livre.
- **Controle:** Separando o Prompt do Sistema da execuÃ§Ã£o da ferramenta.

#### [MÃ³dulo 04: LangGraph (O CoraÃ§Ã£o)](./03-ai-agents/04-langgraph-orchestration)
- **State Machines:** Por que abandonamos "Chains" e usamos "Grafos de Estado".
- **Controle de Fluxo:** Loops, condicionais, retries e persistÃªncia de estado.
- **OrquestraÃ§Ã£o:** Como desenhar um fluxo que se recupera de erros sozinho.

#### [MÃ³dulo 05: Sistemas de MemÃ³ria](./03-ai-agents/05-memory-systems)
- **Short-term:** O contexto da conversa atual.
- **Long-term:** Usando Vector DBs para lembrar preferÃªncias do usuÃ¡rio meses depois.
- **Engenharia:** MemÃ³ria como um problema de Engenharia de Dados, nÃ£o de prompt.

#### [MÃ³dulo 06: MCP (Model Context Protocol)](./03-ai-agents/06-mcp-protocol)
- **O Novo PadrÃ£o:** Padronizando como IAs se conectam a dados (Slack, GitHub, Postgres).
- **Desacoplamento:** Trocando o modelo sem quebrar a integraÃ§Ã£o com as ferramentas.

#### [MÃ³dulo 07: Single-Agent vs Multi-Agent](./03-ai-agents/07-multi-agent-systems)
- **O Mito:** "Mais agentes = Melhor". (Geralmente Ã© mentira).
- **PadrÃµes de DelegaÃ§Ã£o:** Supervisor, HierÃ¡rquico e Colaborativo.
- **Custo:** Como sistemas multi-agente multiplicam latÃªncia e tokens.

#### [MÃ³dulo 08: AvaliaÃ§Ã£o & SeguranÃ§a](./03-ai-agents/08-safety-evals)
- **Perigos:** Loops infinitos, AlucinaÃ§Ã£o de Tools, Prompt Injection.
- **Guardrails:** Colocando cercas elÃ©tricas em volta do agente.
- **Timeouts:** Nunca deixe um agente rodar para sempre.

#### [MÃ³dulo 09: Human-in-the-Loop](./03-ai-agents/09-human-in-the-loop)
- **AprovaÃ§Ã£o:** O agente *propÃµe* uma aÃ§Ã£o (enviar email), o humano *aprova*.
- **InterrupÃ§Ã£o:** Como pausar o grafo e esperar input do usuÃ¡rio.
- **Auditoria:** Quem autorizou essa transaÃ§Ã£o?

#### [MÃ³dulo 10: Agentes em ProduÃ§Ã£o](./03-ai-agents/10-agents-in-production)
- **Observabilidade:** Rastreando o pensamento do agente passo-a-passo (Langfuse).
- **Versionamento:** Como fazer deploy de uma nova versÃ£o do "cÃ©rebro".
- **Rollback:** O que fazer quando o agente enlouquece sexta-feira Ã  noite.

---

### ğŸ› ï¸ Stack de Agentes (PadrÃ£o 2025)

| Componente | Escolha | Por quÃª? |
|:---|:---|:---|
| **OrquestraÃ§Ã£o** | LangGraph | Controle de estado, loops e persistÃªncia nativa. |
| **DefiniÃ§Ã£o de Tools** | Pydantic v2 | ValidaÃ§Ã£o rigorosa de input/output. |
| **Modelo** | GPT-4o / Claude 3.5 Sonnet | Modelos "inteligentes" sÃ£o obrigatÃ³rios para agentes complexos. |
| **MemÃ³ria** | Redis / Postgres | PersistÃªncia de estado rÃ¡pida e confiÃ¡vel. |
| **Protocolo** | MCP | Para conectar com ferramentas externas de forma padronizada. |
| **Tracing** | Langfuse | Visualizar o loop de pensamento Ã© vital. |

### ğŸ§  MudanÃ§as Mentais NecessÃ¡rias
- **Determinismo Morreu:** Agentes sÃ£o probabilÃ­sticos. Seu cÃ³digo precisa lidar com incerteza.
- **Mais CÃ³digo, Menos Prompt:** A lÃ³gica de controle deve estar em Python (Edges do Grafo), nÃ£o no Prompt.
- **Falha Ã© o PadrÃ£o:** O agente VAI errar. O sistema deve ser desenhado para se recuperar.

### ğŸ“š ReferÃªncias Recomendadas
* [Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction): Curso popular sobre agentes da Hugging Face.
* [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/): Como construir agentes com LangGraph.
* [LlamaIndex Agents](https://docs.llamaindex.ai/en/stable/use_cases/agents/): Agentes com LlamaIndex.

---

### [ğŸ”¹ Bloco 4: Infraestrutura & Modelos](./04-infra-ocr-models)
Onde a engenharia de software encontra o "Metal". Rodando modelos de forma eficiente e barata.

#### [MÃ³dulo 01: Ecossistema Moderno de Modelos](./04-infra-ocr-models/01-model-ecosystem)
- **DecisÃ£o:** API ProprietÃ¡ria (OpenAI/Anthropic) vs Open Source (Llama/Mistral).
- **CritÃ©rios:** Privacidade, LatÃªncia, Custo e Complexidade Operacional.
- **EstratÃ©gia:** "Good Enough" models e padrÃµes de roteamento.

#### [MÃ³dulo 02: Ecossistema Hugging Face](./04-infra-ocr-models/02-hugging-face)
- **Abase:** O que sÃ£o Safetensors, Tokenizers e Transformers na prÃ¡tica.
- **Formatos:** FP16, INT8, GGUF, AWQ. O que usar e quando.
- **Realidade:** Quando o Hugging Face Ã© essencial e quando Ã© complexidade desnecessÃ¡ria.

#### [MÃ³dulo 03: Ollama (Dev Locals)](./04-infra-ocr-models/03-ollama)
- **Prototipagem:** Como rodar Llama 3 no seu MacBook em 5 minutos.
- **Limites:** Por que vocÃª (provavelmente) nÃ£o deve usar Ollama em produÃ§Ã£o de alta escala.
- **Workflow:** De local (Ollama) para staging (vLLM).

#### [MÃ³dulo 04: vLLM (InferÃªncia de ProduÃ§Ã£o)](./04-infra-ocr-models/04-vllm)
- **O PadrÃ£o:** Continuous Batching e PagedAttention.
- **Servindo:** Como subir um servidor compatÃ­vel com OpenAI API que aguenta 1000 requests/seg.
- **Tunning:** Ajustando KV Cache e Max Tokens para throughput mÃ¡ximo.

#### [MÃ³dulo 05: Hardware & Performance](./04-infra-ocr-models/05-hardware-performance)
- **VRAM is King:** Por que a memÃ³ria da GPU importa mais que o Compute.
- **Unit Economics:** Quanto custa 1 milhÃ£o de tokens self-hosted vs API?
- **QuantizaÃ§Ã£o:** As trocas entre precisÃ£o e velocidade.

#### [MÃ³dulo 06: Fundamentos de OCR](./04-infra-ocr-models/06-ocr-fundamentals)
- **A Mentira:** OCR nÃ£o Ã© apenas extrair texto. Ã‰ extrair layout, tabelas e estrutura.
- **Desafios:** RotaÃ§Ã£o, ruÃ­do, caligrafia e formataÃ§Ã£o complexa.
- **MÃ©tricas:** Quando CER/WER importam e quando sÃ£o irrelevantes.

#### [MÃ³dulo 07: Frameworks e Pipelines de OCR](./04-infra-ocr-models/07-ocr-pipelines)
- **Ferramentas:** Tesseract vs Azure DI vs Vision LLMs (GPT-4o).
- **Arquitetura:** PrÃ©-processamento, OCR, PÃ³s-processamento e Chunking.
- **Tradeoffs:** Custo (Vision LLM) vs Qualidade vs Velocidade.

#### [MÃ³dulo 08: Document Intelligence em ProduÃ§Ã£o](./04-infra-ocr-models/08-document-intelligence)
- **End-to-End:** IngestÃ£o, Fila (SQS), Processamento Idempotente e IndexaÃ§Ã£o.
- **Falhas:** Dead Level Queues e estratÃ©gias de retry.
- **Monitoramento:** Como saber se o seu pipeline de PDF parou.

---

### ğŸ› ï¸ Stack de Infra (PadrÃ£o 2025)

| Componente | Escolha | Por quÃª? |
|:---|:---|:---|
| **InferÃªncia Local** | Ollama | DX imbatÃ­vel para desenvolvimento. |
| **InferÃªncia Prod** | vLLM | PadrÃ£o ouro para throughput em GPUs NVIDIA. |
| **Model Registry** | Hugging Face | O GitHub dos modelos. |
| **Container** | Docker (NVIDIA Runtime) | Isolamento e portabilidade. |
| **OCR** | HÃ­brido (Layout Parser + Vision LLM) | Melhor custo-benefÃ­cio para documentos complexos. |

### ğŸ§  MudanÃ§as Mentais NecessÃ¡rias
- **GPU nÃ£o Ã© CPU:** O gargalo quase sempre Ã© largura de banda de memÃ³ria (VRAM Bandwidth), nÃ£o FLOPs.
- **Pipeline > Modelo:** Um modelo mÃ©dio com um pipeline de dados excelente bate um modelo state-of-the-art com dados ruins.
- **AssÃ­ncrono Ã© ObrigatÃ³rio:** Modelos sÃ£o lentos. OCR Ã© lento. Se seu sistema for sÃ­ncrono, ele vai cair.

### ğŸ“š ReferÃªncias Recomendadas
* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) por Hugging Face: Otimizando inferÃªncia em GPUs.
* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) por Databricks: Melhores prÃ¡ticas de inferÃªncia.
* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization): QuantizaÃ§Ã£o, Flash Attention, etc.
* [Assisted Generation](https://huggingface.co/blog/assisted-generation): DecodificaÃ§Ã£o especulativa.
* [EAGLE-3 paper](https://arxiv.org/abs/2503.01840?utm_source=chatgpt.com): Paper do EAGLE-3.
* [Speculators](https://github.com/vllm-project/speculators): Biblioteca vLLM para decodificaÃ§Ã£o especulativa.
* [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps): Tutorial de app Streamlit.
* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm): Deploy no SageMaker.
* [Philschmid blog](https://www.philschmid.de/): Artigos sobre deploy de LLMs.
* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/): Vulnerabilidades crÃ­ticas.
* [Prompt Injection Primer](https://github.com/jthack/PIPE): Guia de prompt injection.
* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming): Guia de red teaming da Microsoft.

---

### [ğŸ”¹ Bloco 5: Fine-Tuning](./05-fine-tuning)
Onde a engenharia de software encontra a especializaÃ§Ã£o. Saber quando treinar â€” e principalmente quando NÃƒO treinar.

#### [MÃ³dulo 01: O que Ã© Fine-Tuning (Realmente)](./05-fine-tuning/01-finetuning-concepts)
- **Realidade:** AdaptaÃ§Ã£o de pesos vs InjeÃ§Ã£o de Conhecimento.
- **Mito:** "Vou treinar o modelo nos meus PDFs para ele saber sobre minha empresa." (Spoiler: NÃ£o vai funcionar).
- **Fato:** Fine-Tuning ensina o modelo a FALAR como um mÃ©dico, nÃ£o a SER um mÃ©dico.

#### [MÃ³dulo 02: Fine-Tuning vs RAG vs Prompting](./05-fine-tuning/02-rag-vs-finetuning)
- **Matriz de DecisÃ£o:** O framework definitivo para escolher a abordagem.
- **RAG:** Para fatos novos e dinÃ¢micos.
- **Fine-Tuning:** Para estilo consistente e reduÃ§Ã£o de latÃªncia/custo.
- **Prompting:** Onde vocÃª deve gastar 90% do seu tempo inicial.

#### [MÃ³dulo 03: Tipos de AdaptaÃ§Ã£o](./05-fine-tuning/03-adaptation-types)
- **Full Fine-Tuning:** Por que vocÃª quase nunca vai fazer isso.
- **PEFT / LoRA:** Como treinar modelos gigantes com pouco VRAM.
- **Instruction Tuning:** Ensinando o modelo a seguir ordens.
- **Likelihood Training (DPO/ORPO):** Ensinando o modelo o que vocÃª prefere.

#### [MÃ³dulo 04: Dados sÃ£o o Modelo](./05-fine-tuning/04-data-prep)
- **A Verdade:** O modelo Ã© apenas um espelho dos seus dados.
- **Qualidade > Quantidade:** 100 exemplos perfeitos valem mais que 10.000 exemplos ruins.
- **Instruction Datasets:** Como formatar seus dados corretamente.

#### [MÃ³dulo 05: AvaliaÃ§Ã£o antes do Treino](./05-fine-tuning/05-evaluation)
- **Regra:** Se vocÃª nÃ£o consegue medir, nÃ£o treine.
- **Baselines:** Como saber se o treino piorou o modelo (Catastrophic Forgetting).
- **LLM-as-a-Judge:** Usando GPT-4 para dar nota no seu Llama-3 finetunado.

#### [MÃ³dulo 06: Unsloth (PrÃ¡tico)](./05-fine-tuning/06-unsloth)
- **A Ferramenta:** Por que Unsloth Ã© o padrÃ£o ouro hoje.
- **EficiÃªncia:** Treinando 2x mais rÃ¡pido com 70% menos memÃ³ria.
- **Workflow:** Do notebook para o GGUF/LoRA Adapter.

#### [MÃ³dulo 07: Infra de Treino & Custo Real](./05-fine-tuning/07-training-ops)
- **Hardware:** Quanto de VRAM vocÃª realmente precisa.
- **Spot Instances:** Economizando 70% na AWS/RunPod.
- **Custo Oculto:** O tempo de engenharia para limpar dados vs o custo de GPU.

#### [MÃ³dulo 08: Deploy & InferÃªncia PÃ³s-Treino](./05-fine-tuning/08-deploy-adapters)
- **Adapters:** Como carregar LoRA adapters no vLLM sem duplicar o modelo base.
- **Merge:** Quando fundir os pesos (Mergekit) e quando carregar dinamicamente.
- **Drift:** Monitorando se o modelo "desaprendeu" coisas importantes.

#### [MÃ³dulo 09: Riscos & ManutenÃ§Ã£o](./05-fine-tuning/09-risks-maintenance)
- **Catastrophic Forgetting:** O modelo ficou Ã³timo em SQL, mas esqueceu como falar inglÃªs.
- **ManutenÃ§Ã£o:** Modelo treinado Ã© modelo "congelado". Como atualizar?

#### [MÃ³dulo 10: Enterprise & Gov](./05-fine-tuning/10-enterprise-gov)
- **Compliance:** Quando o Fine-Tuning Ã© obrigatÃ³rio por lei (On-premise total).
- **Privacidade:** Garantindo que dados sensÃ­veis nÃ£o vazem.

---

### ğŸ› ï¸ Stack de Treino (PadrÃ£o 2025)

| Componente | Escolha | Por quÃª? |
|:---|:---|:---|
| **Framework** | Unsloth | Velocidade e eficiÃªncia de memÃ³ria imbatÃ­veis. |
| **TÃ©cnica** | QLoRA (4-bit) | Permite treinar 70B em GPUs "baratas" (A6000/A100). |
| **Eval** | Ragas / LLM-as-Judge | AvaliaÃ§Ã£o escalÃ¡vel antes de deploy. |
| **Dataset** | Hugging Face Datasets | Gerenciamento e versionamento de dados. |

### ğŸ§  MudanÃ§as Mentais NecessÃ¡rias
- **Menos Ã© Mais:** Comece com 50 exemplos. Teste. Se melhorar, adicione mais.
- **Dados sÃ£o CÃ³digo:** Trate seu dataset com o mesmo rigor que trata seu cÃ³digo (versionamento, code review, linting).
- **VocÃª provavelmente nÃ£o precisa de Fine-Tuning:** SÃ©rio. RAG + Few-Shot Prompting resolve 95% dos casos.

---

## ğŸ—ï¸ Arquitetura & Filosofia
Este repositÃ³rio Ã© construÃ­do como um **Monorepo** representando uma Plataforma de IA Enterprise completa.

- **Production-First:** Todo exemplo trata erros, logs e variÃ¡veis de ambiente.
- **EscalÃ¡vel:** Estrutura de pastas que vocÃª veria na Netflix, Uber ou startups de alto crescimento.
- **Opinativo:** Escolhemos o stack que *funciona* (ex: Pydantic sobre dataclasses, FastAPI sobre Flask).

> **"Amadores falam sobre algoritmos. Profissionais falam sobre logÃ­stica (infraestrutura, custo, latÃªncia)."**

---

## ğŸš€ Como ComeÃ§ar

1. **Clone o repositÃ³rio:**
   ```bash
   git clone https://github.com/seususuario/ai-engineer-roadmap.git
   cd ai-engineer-roadmap
   ```

2. **Configure o ambiente (usando `uv`):**
   ```bash
   # Recomendamos uv pela velocidade
   uv venv
   source .venv/bin/activate  # ou .venv\Scripts\activate no Windows
   uv pip install -r requirements.txt
   ```

3. **Navegue para o Bloco 1:**
   ```bash
   cd 01-foundations
   ```

## ğŸ¤ Contribuindo
Exigimos padrÃµes altos. Este nÃ£o Ã© um lugar para scripts "hello world".

## ğŸ“ LicenÃ§a
MIT. Construa coisas incrÃ­veis. Ganhe dinheiro. Mude o mundo.

## ğŸ™ Agradecimentos

Este roadmap foi inspirado no excelente [DevOps Roadmap](https://github.com/milanm/DevOps-Roadmap) de Milan MilanoviÄ‡ e Romano Roth.

Agradecimentos especiais a:
* Thomas Thelen por me motivar a criar um roadmap
* AndrÃ© Frade por sua contribuiÃ§Ã£o e revisÃ£o do primeiro esboÃ§o
* Dino Dunn por fornecer recursos sobre seguranÃ§a LLM
* Magdalena Kuhn por melhorar a parte de "avaliaÃ§Ã£o humana"
* Odoverdose por sugerir o vÃ­deo de 3Blue1Brown sobre Transformers
* Todos que contribuÃ­ram para as referÃªncias educacionais neste curso :)

*Aviso Legal: Eu nÃ£o sou afiliado a nenhuma fonte listada aqui.*